<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>é­é›¨é£ (Yufei Wei) - ä¸ªäººä¸»é¡µ</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #fff;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
        }

        /* Header Section */
        .profile-header {
            display: grid;
            grid-template-columns: 1fr 250px;
            gap: 40px;
            margin-bottom: 60px;
            padding: 40px 0;
            border-bottom: 1px solid #e0e0e0;
        }

        .profile-info h1 {
            font-size: 2.5em;
            font-weight: 700;
            margin-bottom: 15px;
            color: #1a1a1a;
        }

        .profile-info .title {
            font-size: 1.1em;
            color: #555;
            margin-bottom: 8px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .profile-photo {
            position: relative;
        }

        .profile-photo img {
            width: 100%;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .contact-info {
            margin-top: 20px;
            display: flex;
            gap: 15px;
            align-items: center;
        }

        .contact-info a {
            color: #555;
            text-decoration: none;
            font-size: 1.5em;
            transition: color 0.3s;
        }

        .contact-info a:hover {
            color: #0066cc;
        }

        .wechat-qr {
            display: inline-block;
            width: 100px;
            height: 100px;
            background: #f0f0f0;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: #999;
            font-size: 0.9em;
        }

        /* Section Styles */
        section {
            margin-bottom: 50px;
        }

        section h2 {
            font-size: 1.8em;
            font-weight: 600;
            margin-bottom: 20px;
            color: #1a1a1a;
            border-bottom: 2px solid #0066cc;
            padding-bottom: 10px;
        }

        section h3 {
            font-size: 1.35em;
            font-weight: 600;
            margin-bottom: 10px;
            color: #333;
        }

        section p {
            margin-bottom: 15px;
            text-align: justify;
        }

        /* News Section */
        .news-item {
            margin-bottom: 15px;
            padding-left: 20px;
            border-left: 3px solid #0066cc;
        }

        .news-date {
            font-weight: 600;
            color: #0066cc;
            margin-right: 10px;
        }

        /* Education Section */
        .education-item {
            margin-bottom: 25px;
            padding: 15px;
            background: #f9f9f9;
            border-radius: 6px;
        }

        .education-item h3 {
            color: #0066cc;
        }

        .education-details {
            color: #666;
            margin-top: 5px;
        }

        /* Publications Section */
        .publication-item {
            display: grid;
            grid-template-columns: 200px 1fr;
            gap: 20px;
            margin-bottom: 30px;
            padding: 20px;
            background: #fafafa;
            border-radius: 8px;
            transition: box-shadow 0.3s;
            align-items: center;
        }

        .publication-item:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }

        .publication-image {
            width: 200px;
            height: 150px;
            background: #e0e0e0;
            border-radius: 6px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: #999;
            font-size: 0.9em;
            overflow: hidden;
        }

        .publication-image img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .publication-content h3 {
            font-size: 1.35em;
            margin-bottom: 10px;
        }

        .publication-content h3 a {
            color: #1a1a1a;
            text-decoration: none;
        }

        .publication-content h3 a:hover {
            color: #0066cc;
        }

        .publication-authors {
            color: #666;
            margin-bottom: 8px;
            font-size: 0.95em;
        }

        .publication-venue {
            color: #0066cc;
            font-weight: 600;
            margin-bottom: 10px;
            font-size: 0.95em;
        }

        .publication-description {
            color: #555;
            font-size: 0.95em;
            line-height: 1.5;
        }

        .publication-links {
            margin-top: 10px;
        }

        .publication-links a {
            display: inline-block;
            margin-right: 15px;
            color: #0066cc;
            text-decoration: none;
            font-size: 0.9em;
        }

        .publication-links a:hover {
            text-decoration: underline;
        }

        /* Awards Section */
        .award-category {
            margin-bottom: 30px;
        }

        .award-category h3 {
            color: #0066cc;
            margin-bottom: 15px;
        }

        .award-item, .patent-item {
            margin-bottom: 12px;
            padding: 10px 15px;
            background: #f9f9f9;
            border-left: 3px solid #0066cc;
            border-radius: 4px;
        }

        .award-item strong {
            color: #333;
        }

        /* Projects Section */
        .project-item {
            margin-bottom: 20px;
            padding: 20px;
            background: #f5f5f5;
            border-radius: 8px;
            border-left: 4px solid #0066cc;
        }

        .project-item h3 {
            color: #1a1a1a;
            margin-bottom: 10px;
        }

        .project-item p {
            color: #555;
        }

        /* Beyond Research Section */
        .activity-item {
            margin-bottom: 25px;
            padding: 20px;
            background: #fafafa;
            border-radius: 8px;
        }

        .activity-item h3 {
            color: #0066cc;
            margin-bottom: 10px;
        }

        /* Footer */
        footer {
            text-align: center;
            padding: 30px 0;
            color: #999;
            border-top: 1px solid #e0e0e0;
            margin-top: 60px;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .profile-header {
                grid-template-columns: 1fr;
            }

            .publication-item {
                grid-template-columns: 1fr;
            }

            .publication-image {
                width: 100%;
            }
        }

        .scholar-link {
            display: inline-block;
            margin-left: 10px;
            color: #0066cc;
            text-decoration: none;
            font-size: 0.9em;
        }

        .scholar-link:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Profile Header -->
        <header class="profile-header">
            <div class="profile-info">
                <h1>é­é›¨é£ (Yufei Wei)</h1>
                <div class="title">ğŸ“ PhD Student @ Zhejiang University</div>
                <div class="title">ğŸ« College of Control Science and Engineering</div>
                <div class="title">ğŸ¤– 3D Vision, World Model & Embodied AI Researcher</div>
                
                <div class="contact-info">
                    <img src="wechat_qr.png" alt="WeChat" style="width:100px;height:100px;border-radius:4px;">
                    <a href="mailto:wyf00@zju.edu.cn" title="Email"><i class="fas fa-envelope"></i></a>
                    <a href="https://scholar.google.com/citations?user=68ftKf4AAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
                    <a href="https://github.com/WeiYuFei0217" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
                </div>
            </div>
            
            <div class="profile-photo">
                <img src="profile_photo.jpg" alt="é­é›¨é£" onerror="this.style.display='none'; this.parentElement.innerHTML='<div style=\'width:100%;height:300px;background:#e0e0e0;border-radius:8px;display:flex;align-items:center;justify-content:center;color:#999;\'>ç…§ç‰‡ä½ç½®</div>'">
            </div>
        </header>

        <!-- Biography Section -->
        <section id="biography">
            <h2>Biography</h2>
            <p>
                æˆ‘æ˜¯æµ™æ±Ÿå¤§å­¦æ§åˆ¶ç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢çš„åšå£«ç ”ç©¶ç”Ÿï¼Œå¸ˆä»ç‹è¶Šæ•™æˆå’Œç†Šè“‰æ•™æˆã€‚æˆ‘çš„ç ”ç©¶æ–¹å‘èšç„¦äº3Dè§†è§‰æ„ŸçŸ¥ã€ä¸–ç•Œæ¨¡å‹å’Œå…·èº«æ™ºèƒ½ï¼Œè‡´åŠ›äºè®©æœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œäº¤äº’çœŸå®ä¸–ç•Œã€‚
            </p>
            <p>
                åœ¨æˆ‘çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä¸“æ³¨äºå•ç›®è§†è§‰é‡Œç¨‹è®¡ã€BEVï¼ˆé¸Ÿç°è§†è§’ï¼‰è¡¨å¾ã€è§†è§‰å®šä½ç­‰å…³é”®æŠ€æœ¯ï¼Œå¹¶åœ¨IROSã€IEEE RA-Lç­‰å›½é™…é¡¶çº§ä¼šè®®å’ŒæœŸåˆŠä¸Šå‘è¡¨å¤šç¯‡è®ºæ–‡ã€‚æˆ‘çš„å·¥ä½œæ—¨åœ¨è§£å†³ç§»åŠ¨æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ„ŸçŸ¥ä¸å¯¼èˆªé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é™ä½å°ºåº¦æ¼‚ç§»å’Œæé«˜å®šä½ç²¾åº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚æˆ‘ç›®å‰çš„ç ”ç©¶å…´è¶£é›†ä¸­åœ¨3Dè§†è§‰æ„ŸçŸ¥å’Œä¸–ç•Œæ¨¡å‹æ„å»ºï¼Œæ¬¢è¿äº¤æµåˆä½œã€‚
            </p>
            <p>
                åœ¨æœ¬ç§‘æœŸé—´ï¼Œã€ç«èµ›ï¼ŒPaddleå¼€æºï¼Œé…é€è½¦é¡¹ç›®ï¼Œå®ä¹ ï¼Œåˆ›ä¸šã€‘<!-- æˆ‘æ›¾è·å¾—å…¨å›½å¤§å­¦ç”Ÿå·¥ç¨‹å®è·µä¸åˆ›æ–°èƒ½åŠ›å¤§èµ›æ™ºèƒ½ç½‘è”æ±½è½¦èµ›é“å…¨å›½å† å†›ã€å…¨å›½å¤§å­¦ç”Ÿæ™ºèƒ½æ±½è½¦ç«èµ›ç™¾åº¦æ™ºèƒ½é©¾é©¶ç»„å…¨å›½å† å†›ç­‰äº”é¡¹å›½å®¶çº§ç«èµ›å† å†›ã€‚æˆ‘çƒ­çˆ±å°†ç†è®ºç ”ç©¶åº”ç”¨äºå®é™…é—®é¢˜ï¼Œå¹¶ç§¯æå‚ä¸å¼€æºé¡¹ç›®å’Œç¤¾åŒºå»ºè®¾ã€‚ -->
            </p>
            <p>
                åœ¨ç ”ç©¶ç”ŸæœŸé—´ï¼Œxxxxxx
            </p>
        </section>

        <!-- News Section -->
        <section id="news">
            <h2>News</h2>
            <div class="news-item">
                <span class="news-date">2025.11</span> 
                <span>é¢„å°æœ¬ä¸€ä½œè®ºæ–‡Multi-cam Multi-map Visual Inertial Localizationå‘å¸ƒåœ¨arXivï¼Œæå‡ºå¤šç›¸æœºå¤šåœ°å›¾è§†è§‰æƒ¯æ€§å®šä½ç³»ç»Ÿï¼Œå®ç°é•¿æœŸç¯å¢ƒå˜åŒ–ä¸‹çš„å®æ—¶å› æœä½å§¿ä¼°è®¡</span>
            </div>
            <div class="news-item">
                <span class="news-date">2025.09</span> 
                <span>é¢„å°æœ¬ä¸€ä½œè®ºæ–‡BEV-ODOM2å‘å¸ƒåœ¨arXivï¼Œæ¢ç´¢PV-BEVèåˆå’Œå¯†é›†æµç›‘ç£çš„å•ç›®è§†è§‰é‡Œç¨‹è®¡</span>
            </div>
            <div class="news-item">
                <span class="news-date">2025.02</span> 
                <span>ä¸€ç¯‡ä¸€ä½œè®ºæ–‡è¢«IEEE RA-Læ¥æ”¶ï¼BEV-DWPVO: BEV-based Differentiable Weighted Procrustes for Low Scale-drift Monocular Visual Odometry</span>
            </div>
            <div class="news-item">
                <span class="news-date">2024.10</span> 
                <span>ä¸€ç¯‡ä¸€ä½œè®ºæ–‡åœ¨IROS 2024ä¸Šå‘è¡¨ï¼BEV-ODOM: Reducing scale drift in monocular visual odometry with BEV representation</span>
            </div>
        </section>

        <!-- Education Section -->
        <section id="education">
            <h2>Education</h2>
            <div class="education-item">
                <h3>æµ™æ±Ÿå¤§å­¦ (Zhejiang University)</h3>
                <div class="education-details">
                    <strong>åšå£«ç ”ç©¶ç”Ÿ</strong> | æ§åˆ¶ç§‘å­¦ä¸å·¥ç¨‹ | 2022.09 - 2027.06 (é¢„è®¡)
                </div>
                <div class="education-details">
                    å¯¼å¸ˆ: ç‹è¶Šæ•™æˆã€ç†Šè“‰æ•™æˆ
                </div>
                <div class="education-details">
                    ç ”ç©¶æ–¹å‘: 3Dè§†è§‰æ„ŸçŸ¥ã€ä¸–ç•Œæ¨¡å‹ã€å…·èº«æ™ºèƒ½
                </div>
            </div>
            
            <div class="education-item">
                <h3>åä¸­ç§‘æŠ€å¤§å­¦ (Huazhong University of Science and Technology)</h3>
                <div class="education-details">
                    <strong>å·¥å­¦å­¦å£«</strong> | ç”µå­ä¿¡æ¯å·¥ç¨‹ | 2018.09 - 2022.06
                </div>
                <div class="education-details">
                    ç”µå­ä¿¡æ¯ä¸é€šä¿¡å­¦é™¢
                </div>
            </div>
        </section>

        <!-- Publications Section -->
        <section id="publications">
            <h2>Publications <a href="https://scholar.google.com/citations?user=68ftKf4AAAAJ" target="_blank" class="scholar-link">[My Google Scholar]</a></h2>
            
            <h3 style="margin-top: 30px; margin-bottom: 20px; color: #0066cc;">First-Author Publications</h3>
            
            <div class="publication-item">
                <div class="publication-image">
                    <img src="paper_images/bev_odom2.jpg" alt="BEV-ODOM2" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>BEV-ODOM2'">
                </div>
                <div class="publication-content">
                    <h3><a href="https://arxiv.org/abs/2509.14636" target="_blank">BEV-ODOM2: Enhanced BEV-based Monocular Visual Odometry with PV-BEV Fusion and Dense Flow Supervision for Ground Robots</a></h3>
                    <div class="publication-authors">
                        <strong>Yufei Wei</strong>, Wangtao Lu, Sha Lu, Chenxiao Hu, Fuzhang Han, Rong Xiong, Yue Wang
                    </div>
                    <div class="publication-venue">
                        IEEE Transactions on Intelligent Transportation Systems (T-ITS) - <strong>Under Review</strong>
                    </div>
                    <div class="publication-description">
                        BEV-ODOM2 introduces dense BEV optical flow supervision constructed directly from pose ground truth and PV-BEV dual-branch fusion to address sparse supervision and information loss without additional annotations. Achieving 40% RTE improvement over existing BEV methods, it delivers state-of-the-art performance across multiple datasets while maintaining superior scale consistency. <strong>(Under Review)</strong>
                    </div>
                    <div class="publication-links">
                        <a href="https://arxiv.org/abs/2509.14636" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    </div>
                </div>
            </div>

            <div class="publication-item">
                <div class="publication-image">
                    <img src="paper_images/bev_dwpvo.jpg" alt="BEV-DWPVO" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>BEV-DWPVO'">
                </div>
                <div class="publication-content">
                    <h3><a href="https://arxiv.org/abs/2502.20078" target="_blank">BEV-DWPVO: BEV-based Differentiable Weighted Procrustes for Low Scale-drift Monocular Visual Odometry on Ground</a></h3>
                    <div class="publication-authors">
                        <strong>Yufei Wei</strong>, Sha Lu, Wangtao Lu, Rong Xiong, Yue Wang
                    </div>
                    <div class="publication-venue">
                        IEEE Robotics and Automation Letters (RA-L), 2025
                    </div>
                    <div class="publication-description">
                        BEV-DWPVO leverages unified metric-scaled BEV representation and differentiable weighted Procrustes solver to address scale drift in monocular visual odometry. By simplifying pose estimation to 3-DoF and training end-to-end with only pose supervision, it achieves best average RTE of 8.67% and 6.92% on NCLT and Oxford datasets, delivering superior scale consistency without requiring depth estimation or segmentation supervision.
                    </div>
                    <div class="publication-links">
                        <a href="https://ieeexplore.ieee.org/abstract/document/10909180" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://github.com/WeiYuFei0217/BEV-DWPVO" target="_blank"><i class="fab fa-github"></i> Code</a>
                    </div>
                </div>
            </div>

            <div class="publication-item">
                <div class="publication-image">
                    <img src="paper_images/bev_odom.jpg" alt="BEV-ODOM" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>BEV-ODOM'">
                </div>
                <div class="publication-content">
                    <h3><a href="https://arxiv.org/abs/2411.10195 " target="_blank">BEV-ODOM: Reducing Scale Drift in Monocular Visual Odometry with BEV Representation</a></h3>
                    <div class="publication-authors">
                        <strong>Yufei Wei</strong>, Sha Lu, Fuzhang Han, Rong Xiong, Yue Wang
                    </div>
                    <div class="publication-venue">
                        2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 349-356
                    </div>
                    <div class="publication-description">
                        BEV-ODOM is the first to demonstrate that BEV representation alone, without segmentation or auxiliary tasks, can achieve low scale drift in monocular visual odometry. Using correlation-based feature extraction and 3-DoF pose regression with only pose supervision, it reveals that scale consistency originates from BEV's inherent metric-scaled grid structure rather than side-tasks, providing a simpler and more interpretable approach.
                    </div>
                    <div class="publication-links">
                        <a href="https://ieeexplore.ieee.org/abstract/document/10801996" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    </div>
                </div>
            </div>

            <div class="publication-item">
                <div class="publication-image">
                    <img src="paper_images/multi_cam.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
                </div>
                <div class="publication-content">
                    <h3><a href="https://arxiv.org/abs/2412.04287" target="_blank">Multi-cam Multi-map Visual Inertial Localization: System, Validation and Dataset</a></h3>
                    <div class="publication-authors">
                        <strong>Yufei Wei</strong>, Fuzhang Han, Yanmei Jiao, Zhuqing Zhang, Yiyuan Pan, Wenjun Huang, Li Tang, Huan Yin, Xiaqing Ding, Rong Xiong, Yue Wang
                    </div>
                    <div class="publication-venue">
                        IEEE Transactions on Field Robotics (T-FR) - <strong>Under Review</strong>
                    </div>
                    <div class="publication-description">
                        Robot control requires causal pose estimates without retroactive corrections. This work proposes a multi-camera filter-based VILO system enabling operation across multiple isolated maps without overlap requirements. Deterministic initialization and IMU-aided 2-point minimal solvers maintain robustness under 80%+ outlier rates. To validate the system under long-term appearance changes, we collect a 9-month, 55km campus dataset and propose causality-preserving evaluation metrics aligned with control requirements.
                    </div>
                    <div class="publication-links">
                        <a href="https://arxiv.org/abs/2412.04287" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://github.com/WeiYuFei0217/BEV-DWPVO" target="_blank"><i class="fab fa-github"></i> Code</a>
                    </div>
                </div>
            </div>

<h3 style="margin-top: 40px; margin-bottom: 20px; color: #0066cc;">Co-authored Publications</h3>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/LWT_JFR2026.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>Learning to Tune a Mobile Robot Planner: Formulation, Architecture, and Sim-to-Real</h3>
        <div class="publication-authors">
            Wangtao Lu, Wei Zhang, <strong>Yufei Wei</strong>, Rong Xiong, Chaoqun Wang, Yue Wang
        </div>
        <div class="publication-venue">
            Journal of Field Robotics - <strong>Under Review</strong>
        </div>
        <div class="publication-description">
            This work resolves the architectural bottleneck of learning-based planner tuning by proposing a multi-rate hierarchical framework that decouples navigation into coordinated low-frequency tuning (1Hz), mid-frequency planning (10Hz), and high-frequency control (50Hz) loops. The Cyclic Co-Training strategy enables stable learning of coupled policies, while the Terrain-Adaptive Controller achieves robust zero-shot sim-to-real transfer by maintaining consistent tracking performance across diverse terrains, outperforming state-of-the-art auto-tuning and end-to-end methods. <strong>(Under Review)</strong>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/WF_IROS2025.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>Human-Guided Robotic-Assistance Handheld Continuum Medical Robot System</h3>
        <div class="publication-authors">
            Fei Wang, Changhao Luo, Zexi Zhao, Pingyu Xiang, <strong>Yufei Wei</strong>, Ke Qiu, Yufei Wei, Yue Wang, Rong Xiong and Haojian Lu
        </div>
        <div class="publication-venue">
            2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) - Accepted
        </div>
        <div class="publication-description">
            HRHC presents a human-guided handheld continuum medical robot system that bridges the gap between manual instruments and fully robotic solutions for minimally invasive surgery. Featuring modular design with integrated binocular vision for real-time depth perception, IMU-driven intuitive control, and millimeter-level dexterity in confined spaces, it extends surgeon capabilities while maintaining portability and natural operation. My contribution: sensor system architecture design and software deployment for the multi-modal perception platform.
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/LS_ACAR2025.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>SeqBEV: Bayesian Sequential Localization Using BEV LiDAR Map</h3>
        <div class="publication-authors">
            Sha Lu, <strong>Yufei Wei</strong>, Rong Xiong, Yue Wang
        </div>
        <div class="publication-venue">
            2025 IEEE International Conference on Real-time Computing and Robotics (RCAR), pp. 407-412
        </div>
        <div class="publication-description">
            SeqBEV proposes a robust sequential LiDAR localization framework using compact BEV map representation and Bayesian filtering for challenging sparse map scenarios (50m node spacing). By employing NCC-based template matching for uncertainty-aware pose distribution estimation and recursive Bayesian fusion for temporal consistency, it achieves 23.33% improvement in Recall@1 and reduces localization errors to 0.62m (50th percentile) on the NCLT dataset, demonstrating superior robustness compared to single-shot and descriptor-based methods in feature-insufficient environments.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/11139435" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/LWT_ICRA2025.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>Reinforcement Learning for Adaptive Planner Parameter Tuning: A Perspective on Hierarchical Architecture</h3>
        <div class="publication-authors">
            Wangtao Lu, <strong>Yufei Wei</strong>, Jiadong Xu, Wenhao Jia, Liang Li, Rong Xiong, Yue Wang
        </div>
        <div class="publication-venue">
            2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 3883-3889
        </div>
        <div class="publication-description">
            This work proposes a hierarchical architecture integrating low-frequency parameter tuning (1Hz), mid-frequency planning (10Hz), and high-frequency control (50Hz) for autonomous navigation. An RL-based feedback controller is introduced to minimize tracking errors that cannot be resolved through parameter tuning alone. Through alternating training between the parameter tuning network and controller, the method achieves 98% success rate and 10.2s completion time on BARN Challenge, winning first place. Real-world experiments demonstrate 100% success rate with superior tracking accuracy.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/11128541" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/ZDK_CCC2025.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>Towards Agricultural Vehicle Autonomy: Adaptive PID Control for Precise Path Tracking with Visual-Inertial Localization</h3>
        <div class="publication-authors">
            Dongkun Zhang, <strong>Yufei Wei</strong>, Xujiong Feng, Jinpeng Gan, Zhi Huang, Huanyu Jiang, Zidong Yang, Daxu Zhao, Rong Xiong, Yue Wang
        </div>
        <div class="publication-venue">
            2025 44th Chinese Control Conference (CCC), pp. 4630-4636
        </div>
        <div class="publication-description">
            This work presents a cost-effective autonomous agricultural vehicle system using visual-inertial localization to replace expensive RTK-GPS. The proposed modular design integrates a quad-camera setup with IMU for robust pose estimation and employs an adaptive PID controller based on FOPTD model for precise path tracking. Field experiments demonstrate errors below 4.38cm and over 90% trajectory points within 7.5cm threshold using vision-only localization, validating its feasibility for practical agricultural automation.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/11178414" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/LWT_RAL2025.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>Demonstration Data-Driven Parameter Adjustment for Trajectory Planning in Highly Constrained Environments</h3>
        <div class="publication-authors">
            Wangtao Lu, Lin Chen, Yunkai Wang, <strong>Yufei Wei</strong>, Zifei Wu, Rong Xiong, Yue Wang
        </div>
        <div class="publication-venue">
            IEEE Robotics and Automation Letters (RA-L), 2024
        </div>
        <div class="publication-description">
            This work introduces a demonstration data-driven RL framework using CGAN discriminator to bridge trajectory-space demonstrations and parameter-space learning. Combined with asynchronous controller architecture, the method achieves 5Ã— faster convergence than pure RL (0.6M vs 3M steps) and secures first place in BARN Challenge, with strong sim-to-real transfer capability validated in real-world experiments.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/10749994" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/HFZ_IROS2024.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>VIVO: A Visual-Inertial-Velocity Odometry with Online Calibration in Challenging Condition</h3>
        <div class="publication-authors">
            Fuzhang Han, Shenhan Jia, Jiyu Yu, <strong>Yufei Wei</strong>, Wenjun Huang, Yue Wang, Rong Xiong
        </div>
        <div class="publication-venue">
            2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 8571-8578
        </div>
        <div class="publication-description">
            VIVO proposes a tightly coupled visual-inertial-velocity odometry with online extrinsic calibration, directly fusing high-frequency velocity measurements into MSCKF-based VIO. Validated on both wheeled robots and high-speed quadrupeds (up to 3 m/s), it achieves superior robustness in challenging conditions with lower ATE than OpenVINS/MSF and 2Ã— efficiency over ORB-SLAM3.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/10801486" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/ZH_IROS2024.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>OTVIC: A Dataset with Online Transmission for Vehicle-to-Infrastructure Cooperative 3D Object Detection</h3>
        <div class="publication-authors">
            He Zhu, Yunkai Wang, Quyu Kong, <strong>Yufei Wei</strong>, Xunlong Xia, Bing Deng, Rong Xiong, Yue Wang
        </div>
        <div class="publication-venue">
            2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 10732-10739
        </div>
        <div class="publication-description">
            OTVIC presents the first real-world V2I cooperative perception dataset with online transmission, capturing dynamic delays, high-speed scenarios (70-110 km/h), and bandwidth constraints in highway environments. The proposed LfFormer framework achieves robust late fusion by encoding infrastructure perception results as transformer queries, delivering 2.3% mAP improvement while maintaining low communication bandwidth and strong robustness to delays and packet loss.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/10802656" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/YHX_ICRA2024.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>Adapting for Calibration Disturbances: A Neural Uncalibrated Visual Servoing Policy</h3>
        <div class="publication-authors">
            Hongxiang Yu, Anzhe Chen, Kechun Xu, Dashun Guo, <strong>Yufei Wei</strong>, Zhongxiang Zhou, Xuebo Zhang, Yue Wang, Rong Xiong
        </div>
        <div class="publication-venue">
            2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 17417-17423
        </div>
        <div class="publication-description">
            NUVS addresses the labor-intensive calibration challenge in industrial visual servoing by proposing a neural uncalibrated policy that adapts to camera calibration disturbances. By estimating calibration embeddings from past observations to modulate the neural controller and leveraging PBVS supervision in simulation, it achieves both the disturbance adaptation of classical methods and the large convergence basin of learning-based approaches, outperforming IBUVS under calibration disturbances with large initial pose offsets.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/10610364" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

        <!-- Awards, Patents & Projects Section -->
        <section id="awards">
            <h2>ç«èµ›è·å¥–ã€ä¸“åˆ©ä¸é¡¹ç›®</h2>
            
            <div class="award-category">
                <h3>ğŸ† ç«èµ›è·å¥–</h3>
                <div class="award-item">
                    <strong>å…¨å›½å† å†›&é˜Ÿé•¿</strong> - 2021å¹´å…¨å›½å¤§å­¦ç”Ÿå·¥ç¨‹å®è·µä¸åˆ›æ–°èƒ½åŠ›å¤§èµ› æ™ºèƒ½ç½‘è”æ±½è½¦è®¾è®¡èµ›é“
                </div>
                <div class="award-item">
                    <strong>å…¨å›½å† å†›&é˜Ÿé•¿</strong> - ç¬¬åäº”å±Šå…¨å›½å¤§å­¦ç”Ÿæ™ºèƒ½æ±½è½¦ç«èµ› ç™¾åº¦æ™ºèƒ½é©¾é©¶ç»„
                </div>
                <div class="award-item">
                    <strong>å…¨å›½å† å†›&é˜Ÿé•¿</strong> - ç¬¬ä¸‰å±Šä¸­å›½é«˜æ ¡æ™ºèƒ½æœºå™¨äººåˆ›æ„å¤§èµ› ROSæ— äººæœºç»„
                </div>
                <div class="award-item">
                    <strong>å…¨å›½ä¸€ç­‰å¥–&é˜Ÿé•¿</strong> - ç¬¬äºŒåä¸‰å±Šä¸­å›½æœºå™¨äººåŠäººå·¥æ™ºèƒ½å¤§èµ› æ— äººé©¾é©¶æŒ‘æˆ˜èµ› & æ·±åº¦å­¦ä¹ æ™ºèƒ½è½¦ï¼ˆåŒèµ›é¡¹ï¼‰
                </div>
                <div class="award-item">
                    <strong>çœèµ›ä¸€ç­‰å¥–&é˜Ÿé•¿</strong> - 2021"è¥¿é—¨å­æ¯"ä¸­å›½æ™ºèƒ½åˆ¶é€ æŒ‘æˆ˜èµ› ç¦»æ•£è¡Œä¸šè‡ªåŠ¨åŒ–
                </div>
                <div class="award-item">
                    <strong>å† å†›&é˜Ÿé•¿</strong> - 2021"å¾®æ´¾Â·ç§å­æ¯"åˆ›æ–°æ€§è½¯ä»¶ç¼–ç¨‹PKèµ›
                </div>
                <div class="award-item">
                    <strong>å† å†›&é˜Ÿé•¿</strong> - åä¸­ç§‘æŠ€å¤§å­¦ç¬¬åå››å±Šç‘è¨æ¯æ™ºèƒ½è½¦å¤§èµ›
                </div>
                <div class="award-item">
                    <strong>äºŒç­‰å¥–</strong> - å…¨å›½å¤§å­¦ç”Ÿç”µå·¥æ•°å­¦å»ºæ¨¡ã€MathorCupé«˜æ ¡æ•°å­¦å»ºæ¨¡ã€"åä¸­æ¯"å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡
                </div>
            </div>

            <div class="award-category">
                <h3>ğŸ“œ å‘æ˜ä¸“åˆ©</h3>
                <div class="patent-item">
                    <strong>CN202411248996.8</strong> - åŸºäºé¸Ÿç°è§†è§’è¡¨å¾çš„å•ç›®è§†è§‰é‡Œç¨‹è®¡ç³»ç»ŸåŠå…¶æ–¹æ³•ï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
                </div>
                <div class="patent-item">
                    <strong>CN202411981529.6</strong> - åŸºäºé¸Ÿç°è§†è§’è¡¨å¾å’Œå¯å¾®åˆ†åŠ æƒProcrustesæ±‚è§£å™¨çš„å•ç›®è§†è§‰é‡Œç¨‹è®¡æ–¹æ³•å’Œç³»ç»Ÿï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
                </div>
                <div class="patent-item">
                    <strong>CN202110568966.5</strong> - åŸºäºåŒç›®è§†è§‰å’Œæ·±åº¦å­¦ä¹ çš„è½¦è¾†é¿éšœæ–¹æ³•ä¸ç”µå­è®¾å¤‡ï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
                </div>
                <div class="patent-item">
                    <strong>CN202110971772.X</strong> - ä¸€ç§åŸºäºæ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„é«˜é€Ÿè¿åŠ¨è½¦è¾†æ§åˆ¶æ–¹æ³•ï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
                </div>
                <div class="patent-item">
                    <strong>CN202110979800.2</strong> - ä¸€ç§åŒé˜¶æ®µè‡ªåŠ¨é©¾é©¶è½¦è¾†è°ƒå¤´è½¨è¿¹è§„åˆ’æ–¹æ³•ï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
                </div>
            </div>

            <div class="award-category">
                <h3>ğŸš€ æœ¬ç§‘ç”Ÿåˆ›ä¸šé¡¹ç›®</h3>
                <div class="patent-item">
                    <strong>çœçº§å¤§åˆ›</strong> - "åŸºäºåŒç›®è§†è§‰å’Œæ·±åº¦å­¦ä¹ çš„è½»é‡çº§æ— äººé…é€è½¦"ï¼ˆåˆåˆ›é¡¹ç›®ï¼Œä¼˜ç§€ç»“é¢˜ï¼‰
                </div>
            </div>
        </section>

        <!-- Internship Experience Section -->
        <section id="internship">
            <h2>Internship Experience</h2>
            
            <div class="activity-item">
                <h3>ğŸ¢ æ·±åœ³ç§‘åˆ›å­¦é™¢æ™ºèƒ½é©¾é©¶ä¸­å¿ƒ (2022.2 - 2022.9)</h3>
                <p>
                    åœ¨æ™ºèƒ½é©¾é©¶ä¸­å¿ƒæ‹…ä»»å®ä¹ ç ”ç©¶å‘˜ï¼Œå‚ä¸è‡ªåŠ¨é©¾é©¶ç›¸å…³é¡¹ç›®çš„ç ”å‘å·¥ä½œã€‚æœŸé—´æ·±å…¥å­¦ä¹ äº†æ™ºèƒ½é©¾é©¶ç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œå°¤å…¶æ˜¯æ¿€å…‰SLAMçš„å®è½¦éƒ¨ç½²åº”ç”¨ï¼Œç§¯ç´¯äº†ä»ç†è®ºåˆ°å®è·µçš„å®è´µç»éªŒã€‚
                </p>
            </div>
        </section>

        <!-- Research Projects Section -->
        <section id="research-projects">
            <h2>Research Projects</h2>
            
            <div class="activity-item">
                <h3>ğŸš— 501ç ”ç©¶æ‰€é¡¹ç›® - åº”ç”¨äº**æ¢æµ‹çš„å¤šä¼ æ„Ÿå™¨èåˆç³»ç»Ÿ</h3>
                <p>
                    è´Ÿè´£é¡¹ç›®çš„å…¨æ ˆæŠ€æœ¯å®ç°ï¼ŒåŒ…æ‹¬ç¡¬ä»¶ç»“æ„è®¾è®¡ã€å¤šä¼ æ„Ÿå™¨æ„ŸçŸ¥ç®—æ³•å¼€å‘ã€SLAMç³»ç»Ÿæ„å»ºä»¥åŠä¸‰ç»´é‡å»ºç­‰æ ¸å¿ƒæ¨¡å—çš„ç ”å‘ä¸é›†æˆã€‚
                </p>
            </div>
            
            <div class="activity-item">
                <h3>ğŸŒ¾ å†œæœºè‡ªåŠ¨åŒ–é¡¹ç›®</h3>
                <p>
                    è´Ÿè´£å†œæœºè‡ªåŠ¨åŒ–é¡¹ç›®-çº¯è§†è§‰æŠ€æœ¯è·¯çº¿ï¼ŒåŒ…æ‹¬ç¡¬ä»¶ç»“æ„è®¾è®¡ã€SLAMå’Œå¯¼èˆªç®—æ³•å®ç°ï¼Œå®ç‰©æ ·æœºæ­å»ºå’Œç”°é—´å®éªŒéªŒè¯çš„å…¨æµç¨‹å·¥ä½œã€‚
                </p>
            </div>
            
            <div class="activity-item">
                <h3>ğŸš™ é˜¿é‡Œé‡‡é›†è½¦é¡¹ç›®</h3>
                <p>
                    è´Ÿè´£æ•°æ®é‡‡é›†è½¦çš„æ„ŸçŸ¥éƒ¨ä»¶ç»“æ„è®¾è®¡ä¸å®è½¦é›†æˆï¼Œä»¥åŠå¤šä¼ æ„Ÿå™¨æ ‡å®šæ–¹æ¡ˆçš„åˆ¶å®šä¸å®æ–½ï¼Œä¸ºé«˜ç²¾åº¦åœ°å›¾æ•°æ®é‡‡é›†æä¾›æŠ€æœ¯æ”¯æ’‘ã€‚
                </p>
            </div>
            
            <div class="activity-item">
                <h3>ğŸ¤– å±±ä¸œå¤§å­¦æœºå™¨ç‹—é›†ç¾¤é¡¹ç›®</h3>
                <p>
                    è´Ÿè´£å››è¶³æœºå™¨äººé›†ç¾¤ç³»ç»Ÿçš„ç¡¬ä»¶å¹³å°æ­å»ºï¼Œä»¥åŠåŸºäºè§†è§‰å’Œæ¿€å…‰é›·è¾¾çš„SLAMç®—æ³•å¼€å‘ï¼Œå®ç°å¤šæœºå™¨äººååŒå®šä½ä¸å¯¼èˆªåŠŸèƒ½ã€‚
                </p>
            </div>
    
            <div class="activity-item">
                <h3>ğŸ¦¾ æµ™æ±Ÿäººå½¢æœºå™¨äººåˆ›æ–°ä¸­å¿ƒï¼ˆå®æ³¢ï¼‰é¡¹ç›®</h3>
                <p>
                    è´Ÿè´£äººå½¢æœºå™¨äººæ„ŸçŸ¥æ¨¡å—çš„ç»“æ„è®¾è®¡ä¸è°ƒè¯•å·¥ä½œï¼Œé‡ç‚¹å¼€å‘åŸºäºBEVçš„ç¯å¢ƒæ„ŸçŸ¥ç³»ç»Ÿï¼Œæå‡æœºå™¨äººçš„ç©ºé—´ç†è§£ä¸å¯¼èˆªèƒ½åŠ›ã€‚
                </p>
            </div>
        </section>

        <!-- Beyond Research Section -->
        <section id="beyond-research">
            <h2>Beyond Research</h2>
            
            <div class="activity-item">
                <h3>ğŸ« å¹¿è¥¿ç‰æ—-å¤åŸå°å­¦æ”¯æ•™æ´»åŠ¨</h3>
                <p>
                    xxxxxx
                </p>
            </div>
            
            <div class="activity-item">
                <h3>ğŸ’¼ åˆ›ä¸šç»å†</h3>
                <p>
                    xxx
                </p>
            </div>
        </section>

        <!-- Footer -->
        <footer>
            <p>Â© 2025 é­é›¨é£ (Yufei Wei).</p>
            <p>Last updated: November 2025</p>
        </footer>
    </div>
</body>
</html>