<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>é­é›¨é£ (Yufei Wei) - ä¸ªäººä¸»é¡µ</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #fff;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
        }

        /* Header Section */
        .profile-header {
            display: grid;
            grid-template-columns: 1fr 250px;
            gap: 40px;
            margin-bottom: 60px;
            padding: 40px 0;
            border-bottom: 1px solid #e0e0e0;
        }

        .profile-info h1 {
            font-size: 2.5em;
            font-weight: 700;
            margin-bottom: 15px;
            color: #1a1a1a;
        }

        .profile-info .title {
            font-size: 1.1em;
            color: #555;
            margin-bottom: 8px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .profile-photo {
            position: relative;
        }

        .profile-photo img {
            width: 100%;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .contact-info {
            margin-top: 20px;
            display: flex;
            gap: 15px;
            align-items: center;
        }

        .contact-info a {
            color: #555;
            text-decoration: none;
            font-size: 1.5em;
            transition: color 0.3s;
        }

        .contact-info a:hover {
            color: #0066cc;
        }

        .wechat-qr {
            display: inline-block;
            width: 100px;
            height: 100px;
            background: #f0f0f0;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: #999;
            font-size: 0.9em;
        }

        /* Section Styles */
        section {
            margin-bottom: 50px;
        }

        section h2 {
            font-size: 1.8em;
            font-weight: 600;
            margin-bottom: 20px;
            color: #1a1a1a;
            border-bottom: 2px solid #0066cc;
            padding-bottom: 10px;
        }

        section h3 {
            font-size: 1.35em;
            font-weight: 600;
            margin-bottom: 10px;
            color: #333;
        }

        section p {
            margin-bottom: 15px;
            text-align: justify;
        }

        /* News Section */
        .news-item {
            margin-bottom: 15px;
            padding-left: 20px;
            border-left: 3px solid #0066cc;
        }

        .news-date {
            font-weight: 600;
            color: #0066cc;
            margin-right: 10px;
        }

        /* Education Section */
        .education-item {
            margin-bottom: 25px;
            padding: 15px;
            background: #f9f9f9;
            border-radius: 6px;
        }

        .education-item h3 {
            color: #0066cc;
        }

        .education-details {
            color: #666;
            margin-top: 5px;
        }

        /* Publications Section */
        .publication-item {
            display: grid;
            grid-template-columns: 200px 1fr;
            gap: 20px;
            margin-bottom: 30px;
            padding: 20px;
            background: #fafafa;
            border-radius: 8px;
            transition: box-shadow 0.3s;
            align-items: center;
        }

        .publication-item:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }

        .publication-image {
            width: 200px;
            height: 150px;
            background: #e0e0e0;
            border-radius: 6px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: #999;
            font-size: 0.9em;
            overflow: hidden;
        }

        .publication-image img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .publication-content h3 {
            font-size: 1.35em;
            margin-bottom: 10px;
        }

        .publication-content h3 a {
            color: #1a1a1a;
            text-decoration: none;
        }

        .publication-content h3 a:hover {
            color: #0066cc;
        }

        .publication-authors {
            color: #666;
            margin-bottom: 8px;
            font-size: 0.95em;
        }

        .publication-venue {
            color: #0066cc;
            font-weight: 600;
            margin-bottom: 10px;
            font-size: 0.95em;
        }

        .publication-description {
            color: #555;
            font-size: 0.95em;
            line-height: 1.5;
        }

        .publication-links {
            margin-top: 10px;
        }

        .publication-links a {
            display: inline-block;
            margin-right: 15px;
            color: #0066cc;
            text-decoration: none;
            font-size: 0.9em;
        }

        .publication-links a:hover {
            text-decoration: underline;
        }

        /* Awards Section */
        .award-category {
            margin-bottom: 30px;
        }

        .award-category h3 {
            color: #0066cc;
            margin-bottom: 15px;
        }

        .award-item, .patent-item {
            margin-bottom: 12px;
            padding: 10px 15px;
            background: #f9f9f9;
            border-left: 3px solid #0066cc;
            border-radius: 4px;
        }

        .award-item strong {
            color: #333;
        }

        /* Projects Section */
        .project-item {
            margin-bottom: 20px;
            padding: 20px;
            background: #f5f5f5;
            border-radius: 8px;
            border-left: 4px solid #0066cc;
        }

        .project-item h3 {
            color: #1a1a1a;
            margin-bottom: 10px;
        }

        .project-item p {
            color: #555;
        }

        /* Beyond Research Section */
        .activity-item {
            margin-bottom: 25px;
            padding: 20px;
            background: #fafafa;
            border-radius: 8px;
        }

        .activity-item h3 {
            color: #0066cc;
            margin-bottom: 10px;
        }

        /* Footer */
        footer {
            text-align: center;
            padding: 30px 0;
            color: #999;
            border-top: 1px solid #e0e0e0;
            margin-top: 60px;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .profile-header {
                grid-template-columns: 1fr;
            }

            .publication-item {
                grid-template-columns: 1fr;
            }

            .publication-image {
                width: 100%;
            }
        }

        .scholar-link {
            display: inline-block;
            margin-left: 10px;
            color: #0066cc;
            text-decoration: none;
            font-size: 0.9em;
        }

        .scholar-link:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Profile Header -->
        <header class="profile-header">
            <div class="profile-info">
                <h1>é­é›¨é£ (Yufei Wei)</h1>
                <div class="title">ğŸ“ PhD Student @ Zhejiang University</div>
                <div class="title">ğŸ« College of Control Science and Engineering</div>
                <div class="title">ğŸ¤– 3D Vision, World Model & Embodied AI Researcher</div>
                
                <div class="contact-info">
                    <img src="wechat_qr.png" alt="WeChat" style="width:100px;height:100px;border-radius:4px;">
                    <a href="mailto:wyf00@zju.edu.cn" title="Email"><i class="fas fa-envelope"></i></a>
                    <a href="https://scholar.google.com/citations?user=68ftKf4AAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
                    <a href="https://github.com/WeiYuFei0217" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
                </div>
            </div>
            
            <div class="profile-photo">
                <img src="profile_photo.jpg" alt="é­é›¨é£" onerror="this.style.display='none'; this.parentElement.innerHTML='<div style=\'width:100%;height:300px;background:#e0e0e0;border-radius:8px;display:flex;align-items:center;justify-content:center;color:#999;\'>ç…§ç‰‡ä½ç½®</div>'">
            </div>
        </header>

        <!-- Biography Section -->
        <section id="biography">
            <h2>Biography</h2>
            <p>Hi! I'm Yufei Wei (é­é›¨é£), a Ph.D. student at the College of Control Science and Engineering, Zhejiang University, advised by Prof. Yue Wang and Prof. Rong Xiong. My research centers on 3D visual perception, world models, and embodied AI â€” I'm driven by the vision of enabling robots to genuinely understand and interact with the real world, not just move through it.</p>
            <p>My earlier work focused on monocular visual odometry, Bird's-Eye View representations, and visual localization, published in journals and conferences including IEEE RA-L and IROS. These efforts addressed real-world challenges in reducing scale drift and achieving robust localization for mobile robots in complex environments. As a collaborator, I've also contributed to projects spanning learning-based navigation planning, multi-sensor fusion localization, and applications in agricultural automation and medical robotics. Currently, I'm exploring world model construction and vision-and-language navigation, focusing on cross-modal integration through unified representations, and leveraging multimodal LLMs for scene understanding and navigation tasks.</p>
            <p>During my undergraduate years, I led teams to three national championships in autonomous driving and robotics competitions, contributed to the PaddlePaddle open-source community, and developed a lightweight autonomous delivery vehicle. I also served as team leader for an intelligent ergonomic chair startup at Zexiang Li's Entrepreneurship Incubator in Shenzhen and spent time teaching at a primary school in Yulin, Guangxi. During my internship at Shenzhen InnoX Academy, I contributed to developing a semantic line segment mapping-based LiDAR-IMU SLAM system and obstacle avoidance algorithms for autonomous street sweeping vehicles. Whether in research or real-world deployment, I prioritize building systems that actually work over chasing theoretical elegance alone.</p>
            <p>I'm always excited to collaborate, discuss ideas, and learn from others. If you're working on visual perception, world models, embodied AI, or anything that brings robots closer to understanding our world, let's chat! ğŸ¤–</p>
        </section>

<!-- News Section -->
        <section id="news">
            <h2>News</h2>
            <div class="news-item">
                <span class="news-date">2025.11</span> 
                <span>First-author preprint <strong>Multi-cam Multi-map Visual Inertial Localization</strong> released on arXiv, proposing a multi-camera multi-map visual-inertial localization system with causality-preserving evaluation metrics.</span>
            </div>
            <div class="news-item">
                <span class="news-date">2025.09</span> 
                <span>First-author preprint <strong>BEV-ODOM2</strong> released on arXiv, exploring PV-BEV fusion and dense flow supervision for BEV-based 3-DoF monocular visual odometry.</span>
            </div>
            <div class="news-item">
                <span class="news-date">2025.02</span> 
                <span>First-author paper <strong>BEV-DWPVO</strong> accepted by <strong>IEEE RA-L</strong>, achieving best average RTE with superior scale consistency using BEV representation and differentiable weighted Procrustes solver.</span>
            </div>
            <div class="news-item">
                <span class="news-date">2024.10</span> 
                <span>First-author paper <strong>BEV-ODOM</strong> published at <strong>IROS 2024</strong>, demonstrating that BEV representation alone can achieve low scale drift in monocular visual odometry.</span>
            </div>
        </section>

        <!-- Education Section -->
        <section id="education">
            <h2>Education</h2>
            <div class="education-item">
                <h3>Zhejiang University (æµ™æ±Ÿå¤§å­¦)</h3>
                <div class="education-details">
                    <strong>Ph.D. Student</strong> | Control Science and Engineering | 2022.09 - 2027.06 (Expected)
                </div>
                <div class="education-details">
                    Advisor: Prof. Yue Wang (ç‹è¶Šæ•™æˆ), Prof. Rong Xiong (ç†Šè“‰æ•™æˆ)
                </div>
                <div class="education-details">
                    Research Interests: 3D Visual Perception, World Models, Embodied AI
                </div>
            </div>
            
            <div class="education-item">
                <h3>Huazhong University of Science and Technology (åä¸­ç§‘æŠ€å¤§å­¦)</h3>
                <div class="education-details">
                    <strong>B.Eng.</strong> | Electronic Information Engineering | 2018.09 - 2022.06
                </div>
                <div class="education-details">
                    School of Electronic Information and Communications
                </div>
            </div>
        </section>

        <!-- Publications Section -->
        <section id="publications">
            <h2>Publications <a href="https://scholar.google.com/citations?user=68ftKf4AAAAJ" target="_blank" class="scholar-link">[My Google Scholar]</a></h2>
            
            <h3 style="margin-top: 30px; margin-bottom: 20px; color: #0066cc;">First-Author Publications</h3>
            
            <div class="publication-item">
                <div class="publication-image">
                    <img src="paper_images/bev_odom2.jpg" alt="BEV-ODOM2" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>BEV-ODOM2'">
                </div>
                <div class="publication-content">
                    <h3><a href="https://arxiv.org/abs/2509.14636" target="_blank">BEV-ODOM2: Enhanced BEV-based Monocular Visual Odometry with PV-BEV Fusion and Dense Flow Supervision for Ground Robots</a></h3>
                    <div class="publication-authors">
                        <strong>Yufei Wei</strong>, Wangtao Lu, Sha Lu, Chenxiao Hu, Fuzhang Han, Rong Xiong, Yue Wang
                    </div>
                    <div class="publication-venue">
                        IEEE Transactions on Intelligent Transportation Systems (T-ITS) - <strong>Under Review</strong>
                    </div>
                    <div class="publication-description">
                        BEV-ODOM2 introduces dense BEV optical flow supervision constructed directly from pose ground truth and PV-BEV dual-branch fusion to address sparse supervision and information loss without additional annotations. Achieving 40% RTE improvement over existing BEV methods, it delivers state-of-the-art performance across multiple datasets while maintaining superior scale consistency. <strong>(Under Review)</strong>
                    </div>
                    <div class="publication-links">
                        <a href="https://arxiv.org/abs/2509.14636" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    </div>
                </div>
            </div>

            <div class="publication-item">
                <div class="publication-image">
                    <img src="paper_images/bev_dwpvo.jpg" alt="BEV-DWPVO" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>BEV-DWPVO'">
                </div>
                <div class="publication-content">
                    <h3><a href="https://arxiv.org/abs/2502.20078" target="_blank">BEV-DWPVO: BEV-based Differentiable Weighted Procrustes for Low Scale-drift Monocular Visual Odometry on Ground</a></h3>
                    <div class="publication-authors">
                        <strong>Yufei Wei</strong>, Sha Lu, Wangtao Lu, Rong Xiong, Yue Wang
                    </div>
                    <div class="publication-venue">
                        IEEE Robotics and Automation Letters (RA-L), 2025
                    </div>
                    <div class="publication-description">
                        BEV-DWPVO leverages unified metric-scaled BEV representation and differentiable weighted Procrustes solver to address scale drift in monocular visual odometry. By simplifying pose estimation to 3-DoF and training end-to-end with only pose supervision, it achieves best average RTE of 8.67% and 6.92% on NCLT and Oxford datasets, delivering superior scale consistency without requiring depth estimation or segmentation supervision.
                    </div>
                    <div class="publication-links">
                        <a href="https://ieeexplore.ieee.org/abstract/document/10909180" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://github.com/WeiYuFei0217/BEV-DWPVO" target="_blank"><i class="fab fa-github"></i> Code</a>
                    </div>
                </div>
            </div>

            <div class="publication-item">
                <div class="publication-image">
                    <img src="paper_images/bev_odom.jpg" alt="BEV-ODOM" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>BEV-ODOM'">
                </div>
                <div class="publication-content">
                    <h3><a href="https://arxiv.org/abs/2411.10195 " target="_blank">BEV-ODOM: Reducing Scale Drift in Monocular Visual Odometry with BEV Representation</a></h3>
                    <div class="publication-authors">
                        <strong>Yufei Wei</strong>, Sha Lu, Fuzhang Han, Rong Xiong, Yue Wang
                    </div>
                    <div class="publication-venue">
                        2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 349-356
                    </div>
                    <div class="publication-description">
                        BEV-ODOM is the first to demonstrate that BEV representation alone, without segmentation or auxiliary tasks, can achieve low scale drift in monocular visual odometry. Using correlation-based feature extraction and 3-DoF pose regression with only pose supervision, it reveals that scale consistency originates from BEV's inherent metric-scaled grid structure rather than side-tasks, providing a simpler and more interpretable approach.
                    </div>
                    <div class="publication-links">
                        <a href="https://ieeexplore.ieee.org/abstract/document/10801996" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    </div>
                </div>
            </div>

            <div class="publication-item">
                <div class="publication-image">
                    <img src="paper_images/multi_cam.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
                </div>
                <div class="publication-content">
                    <h3><a href="https://arxiv.org/abs/2412.04287" target="_blank">Multi-cam Multi-map Visual Inertial Localization: System, Validation and Dataset</a></h3>
                    <div class="publication-authors">
                        <strong>Yufei Wei</strong>, Fuzhang Han, Yanmei Jiao, Zhuqing Zhang, Yiyuan Pan, Wenjun Huang, Li Tang, Huan Yin, Xiaqing Ding, Rong Xiong, Yue Wang
                    </div>
                    <div class="publication-venue">
                        IEEE Transactions on Field Robotics (T-FR) - <strong>Under Review</strong>
                    </div>
                    <div class="publication-description">
                        Robot control requires causal pose estimates without retroactive corrections. This work proposes a multi-camera filter-based VILO system enabling operation across multiple isolated maps without overlap requirements. Deterministic initialization and IMU-aided 2-point minimal solvers maintain robustness under 80%+ outlier rates. To validate the system under long-term appearance changes, we collect a 9-month, 55km campus dataset and propose causality-preserving evaluation metrics aligned with control requirements.
                    </div>
                    <div class="publication-links">
                        <a href="https://arxiv.org/abs/2412.04287" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://github.com/WeiYuFei0217/BEV-DWPVO" target="_blank"><i class="fab fa-github"></i> Code</a>
                    </div>
                </div>
            </div>

<h3 style="margin-top: 40px; margin-bottom: 20px; color: #0066cc;">Co-authored Publications</h3>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/LWT_JFR2026.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>Learning to Tune a Mobile Robot Planner: Formulation, Architecture, and Sim-to-Real</h3>
        <div class="publication-authors">
            Wangtao Lu, Wei Zhang, <strong>Yufei Wei</strong>, Rong Xiong, Chaoqun Wang, Yue Wang
        </div>
        <div class="publication-venue">
            Journal of Field Robotics - <strong>Under Review</strong>
        </div>
        <div class="publication-description">
            This work resolves the architectural bottleneck of learning-based planner tuning by proposing a multi-rate hierarchical framework that decouples navigation into coordinated low-frequency tuning (1Hz), mid-frequency planning (10Hz), and high-frequency control (50Hz) loops. The Cyclic Co-Training strategy enables stable learning of coupled policies, while the Terrain-Adaptive Controller achieves robust zero-shot sim-to-real transfer by maintaining consistent tracking performance across diverse terrains, outperforming state-of-the-art auto-tuning and end-to-end methods. <strong>(Under Review)</strong>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/WF_IROS2025.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>Human-Guided Robotic-Assistance Handheld Continuum Medical Robot System</h3>
        <div class="publication-authors">
            Fei Wang, Changhao Luo, Zexi Zhao, Pingyu Xiang, <strong>Yufei Wei</strong>, Ke Qiu, Yufei Wei, Yue Wang, Rong Xiong and Haojian Lu
        </div>
        <div class="publication-venue">
            2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) - Accepted
        </div>
        <div class="publication-description">
            HRHC presents a human-guided handheld continuum medical robot system that bridges the gap between manual instruments and fully robotic solutions for minimally invasive surgery. Featuring modular design with integrated binocular vision for real-time depth perception, IMU-driven intuitive control, and millimeter-level dexterity in confined spaces, it extends surgeon capabilities while maintaining portability and natural operation. My contribution: sensor system architecture design and software deployment for the multi-modal perception platform.
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/LS_ACAR2025.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>SeqBEV: Bayesian Sequential Localization Using BEV LiDAR Map</h3>
        <div class="publication-authors">
            Sha Lu, <strong>Yufei Wei</strong>, Rong Xiong, Yue Wang
        </div>
        <div class="publication-venue">
            2025 IEEE International Conference on Real-time Computing and Robotics (RCAR), pp. 407-412
        </div>
        <div class="publication-description">
            SeqBEV proposes a robust sequential LiDAR localization framework using compact BEV map representation and Bayesian filtering for challenging sparse map scenarios (50m node spacing). By employing NCC-based template matching for uncertainty-aware pose distribution estimation and recursive Bayesian fusion for temporal consistency, it achieves 23.33% improvement in Recall@1 and reduces localization errors to 0.62m (50th percentile) on the NCLT dataset, demonstrating superior robustness compared to single-shot and descriptor-based methods in feature-insufficient environments.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/11139435" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/LWT_ICRA2025.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>Reinforcement Learning for Adaptive Planner Parameter Tuning: A Perspective on Hierarchical Architecture</h3>
        <div class="publication-authors">
            Wangtao Lu, <strong>Yufei Wei</strong>, Jiadong Xu, Wenhao Jia, Liang Li, Rong Xiong, Yue Wang
        </div>
        <div class="publication-venue">
            2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 3883-3889
        </div>
        <div class="publication-description">
            This work proposes a hierarchical architecture integrating low-frequency parameter tuning (1Hz), mid-frequency planning (10Hz), and high-frequency control (50Hz) for autonomous navigation. An RL-based feedback controller is introduced to minimize tracking errors that cannot be resolved through parameter tuning alone. Through alternating training between the parameter tuning network and controller, the method achieves 98% success rate and 10.2s completion time on BARN Challenge, winning first place. Real-world experiments demonstrate 100% success rate with superior tracking accuracy.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/11128541" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/ZDK_CCC2025.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>Towards Agricultural Vehicle Autonomy: Adaptive PID Control for Precise Path Tracking with Visual-Inertial Localization</h3>
        <div class="publication-authors">
            Dongkun Zhang, <strong>Yufei Wei</strong>, Xujiong Feng, Jinpeng Gan, Zhi Huang, Huanyu Jiang, Zidong Yang, Daxu Zhao, Rong Xiong, Yue Wang
        </div>
        <div class="publication-venue">
            2025 44th Chinese Control Conference (CCC), pp. 4630-4636
        </div>
        <div class="publication-description">
            This work presents a cost-effective autonomous agricultural vehicle system using visual-inertial localization to replace expensive RTK-GPS. The proposed modular design integrates a quad-camera setup with IMU for robust pose estimation and employs an adaptive PID controller based on FOPTD model for precise path tracking. Field experiments demonstrate errors below 4.38cm and over 90% trajectory points within 7.5cm threshold using vision-only localization, validating its feasibility for practical agricultural automation.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/11178414" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/LWT_RAL2025.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>Demonstration Data-Driven Parameter Adjustment for Trajectory Planning in Highly Constrained Environments</h3>
        <div class="publication-authors">
            Wangtao Lu, Lin Chen, Yunkai Wang, <strong>Yufei Wei</strong>, Zifei Wu, Rong Xiong, Yue Wang
        </div>
        <div class="publication-venue">
            IEEE Robotics and Automation Letters (RA-L), 2024
        </div>
        <div class="publication-description">
            This work introduces a demonstration data-driven RL framework using CGAN discriminator to bridge trajectory-space demonstrations and parameter-space learning. Combined with asynchronous controller architecture, the method achieves 5Ã— faster convergence than pure RL (0.6M vs 3M steps) and secures first place in BARN Challenge, with strong sim-to-real transfer capability validated in real-world experiments.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/10749994" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/HFZ_IROS2024.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>VIVO: A Visual-Inertial-Velocity Odometry with Online Calibration in Challenging Condition</h3>
        <div class="publication-authors">
            Fuzhang Han, Shenhan Jia, Jiyu Yu, <strong>Yufei Wei</strong>, Wenjun Huang, Yue Wang, Rong Xiong
        </div>
        <div class="publication-venue">
            2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 8571-8578
        </div>
        <div class="publication-description">
            VIVO proposes a tightly coupled visual-inertial-velocity odometry with online extrinsic calibration, directly fusing high-frequency velocity measurements into MSCKF-based VIO. Validated on both wheeled robots and high-speed quadrupeds (up to 3 m/s), it achieves superior robustness in challenging conditions with lower ATE than OpenVINS/MSF and 2Ã— efficiency over ORB-SLAM3.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/10801486" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/ZH_IROS2024.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>OTVIC: A Dataset with Online Transmission for Vehicle-to-Infrastructure Cooperative 3D Object Detection</h3>
        <div class="publication-authors">
            He Zhu, Yunkai Wang, Quyu Kong, <strong>Yufei Wei</strong>, Xunlong Xia, Bing Deng, Rong Xiong, Yue Wang
        </div>
        <div class="publication-venue">
            2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 10732-10739
        </div>
        <div class="publication-description">
            OTVIC presents the first real-world V2I cooperative perception dataset with online transmission, capturing dynamic delays, high-speed scenarios (70-110 km/h), and bandwidth constraints in highway environments. The proposed LfFormer framework achieves robust late fusion by encoding infrastructure perception results as transformer queries, delivering 2.3% mAP improvement while maintaining low communication bandwidth and strong robustness to delays and packet loss.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/10802656" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

<div class="publication-item">
    <div class="publication-image">
        <img src="paper_images/YHX_ICRA2024.jpg" alt="Multi-cam VIL" onerror="this.parentElement.innerHTML='å›¾ç‰‡ä½ç½®<br>Multi-cam'">
    </div>
    <div class="publication-content">
        <h3>Adapting for Calibration Disturbances: A Neural Uncalibrated Visual Servoing Policy</h3>
        <div class="publication-authors">
            Hongxiang Yu, Anzhe Chen, Kechun Xu, Dashun Guo, <strong>Yufei Wei</strong>, Zhongxiang Zhou, Xuebo Zhang, Yue Wang, Rong Xiong
        </div>
        <div class="publication-venue">
            2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 17417-17423
        </div>
        <div class="publication-description">
            NUVS addresses the labor-intensive calibration challenge in industrial visual servoing by proposing a neural uncalibrated policy that adapts to camera calibration disturbances. By estimating calibration embeddings from past observations to modulate the neural controller and leveraging PBVS supervision in simulation, it achieves both the disturbance adaptation of classical methods and the large convergence basin of learning-based approaches, outperforming IBUVS under calibration disturbances with large initial pose offsets.
        </div>
        <div class="publication-links">
            <a href="https://ieeexplore.ieee.org/abstract/document/10610364" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
        </div>
    </div>
</div>

        <!-- Research Projects Section -->
        <section id="research-projects">
            <h2>Research Projects</h2>
            
            <div class="activity-item">
                <h3>ğŸš€ ZJU & xx Research Institute - Legged xx Exploration Robot Project</h3>
                <p>
                    Designed and led the development of a full-stack perception and navigation system for autonomous exploration in complex terrain. Built an integrated multi-sensor stack (LiDAR, stereo cameras, IMU) with a robust calibration pipeline, and implemented laser-inertial / visual-inertial localization together with deep learningâ€“based stereo matching for dense 3D reconstruction. On top of this stack, developed a lightweight terrain analysis and path-planning framework tailored to resource-constrained platforms, operating directly on point clouds to extract ground planes, characterize terrain geometry (concavities/convexities), identify traversable regions, and generate optimized paths via graph-based planning. Validated the system in 100+ automated test scenarios, demonstrating reliable navigation performance in cluttered, highly irregular environments.
                </p>
                <p style="color: #999; font-size: 0.8em; font-style: italic; margin-top: 0.5em;">* xx: redacted for confidentiality</p>
            </div>
            
            <div class="activity-item">
                <h3>ğŸŒ¾ Provincial Key R&D Program - Agricultural Automation System</h3>
                <p>
                    Designed and led the development of a vision-only navigation system for agricultural machinery, covering the complete sensing architecture, perception stack, and visualâ€“inertial odometry framework. Built a four-camera and IMU perception module with a dedicated vibration-attenuating structure and implemented a multi-view visualâ€“inertial fusion system optimized for low-texture, high-vibration agricultural environments. Integrated the system with an error-compensation tracking controller and demonstrated centimeter-level trajectory accuracy in rice-transplanter farmland trials when compared against RTK-INS ground truth.
                </p>
            </div>
            
            <div class="activity-item">
                <h3>ğŸ¦¾ ZJU & Zhejiang Humanoid Robot Innovation Center â€“ Wheeled Humanoid Perception & Navigation System</h3>
                <p>
                    Developed the perception and navigation subsystem for a wheeled humanoid robot platform. Built a vision-centric BEV representation from multi-camera inputs to generate dense costmaps and 3-DoF pose estimates. Integrated the perception layer with A* global planning and TEB local optimization, enabling reliable, obstacle-aware visual navigation for the humanoid robot's wheeled base.
                </p>
            </div>
            
            <div class="activity-item">
                <h3>ğŸš™ ZJU & Alibaba - Autonomous Driving Data Collection Platform</h3>
                <p>
                    Designed the perception hardware architecture for the autonomous driving data collection vehicle, including structural design and vehicle-level integration of the quad-camera, LiDAR, IMU, GPS/RTK, and V2X sensing modules. Implemented the multi-sensor calibration workflow and deployed the multi-view visualâ€“IMUâ€“GPS odometry algorithms on the vehicle platform to ensure accurate and reliable motion estimation. Conducted data consistency checks and ground-truth alignment to support high-quality multi-modal dataset generation.
                </p>
            </div>
            
            <div class="activity-item">
                <h3>ğŸ• ZJU & SDU - Quadruped Robot Swarm Project</h3>
                <p>
                    Deployed stereo-visionâ€“based 3D reconstruction on quadruped platforms and built a BEV-centric spatial representation for robust perception in multi-robot settings. Implemented BEV odometry to generate reliable pose estimates for downstream planning modules, enabling coordinated motion within the robot swarm.
                </p>
            </div>
        </section>

        <!-- Internship & Industrial Experience Section -->
        <section id="experience">
            <h2>Industrial Experience</h2>
            
            <div class="activity-item">
                <h3>ğŸ¢ Shenzhen InnoX Academy - Autonomous Driving Center (2022.2 - 2022.9)</h3>
                <p>
                    Built a semantic line segment mapping system for autonomous street sweeping vehicles in urban environments. The system uses deep semantic segmentation to extract static linear landmarks like lamp posts and road markings. By representing these landmarks as compact line segments rather than dense point clouds, it filters out dynamic traffic participants, handles varying weather and lighting conditions, and enables real-time LiDAR-IMU SLAM on resource-constrained onboard computers.
                </p>
            </div>
        </section>

        <!-- Awards, Patents & Projects Section -->
        <section id="awards">
            <h2>ç«èµ›è·å¥–ã€ä¸“åˆ©ä¸é¡¹ç›®</h2>
            
            <div class="award-category">
                <h3>ğŸ† ç«èµ›è·å¥–</h3>
                <div class="award-item">
                    <strong>å…¨å›½å† å†›&é˜Ÿé•¿</strong> - 2021å¹´å…¨å›½å¤§å­¦ç”Ÿå·¥ç¨‹å®è·µä¸åˆ›æ–°èƒ½åŠ›å¤§èµ› æ™ºèƒ½ç½‘è”æ±½è½¦è®¾è®¡èµ›é“
                </div>
                <div class="award-item">
                    <strong>å…¨å›½å† å†›&é˜Ÿé•¿</strong> - ç¬¬åäº”å±Šå…¨å›½å¤§å­¦ç”Ÿæ™ºèƒ½æ±½è½¦ç«èµ› ç™¾åº¦æ™ºèƒ½é©¾é©¶ç»„
                </div>
                <div class="award-item">
                    <strong>å…¨å›½å† å†›&é˜Ÿé•¿</strong> - ç¬¬ä¸‰å±Šä¸­å›½é«˜æ ¡æ™ºèƒ½æœºå™¨äººåˆ›æ„å¤§èµ› ROSæ— äººæœºç»„
                </div>
                <div class="award-item">
                    <strong>å…¨å›½ä¸€ç­‰å¥–&é˜Ÿé•¿</strong> - ç¬¬äºŒåä¸‰å±Šä¸­å›½æœºå™¨äººåŠäººå·¥æ™ºèƒ½å¤§èµ› æ— äººé©¾é©¶æŒ‘æˆ˜èµ› & æ·±åº¦å­¦ä¹ æ™ºèƒ½è½¦ï¼ˆåŒèµ›é¡¹ï¼‰
                </div>
                <div class="award-item">
                    <strong>çœèµ›ä¸€ç­‰å¥–&é˜Ÿé•¿</strong> - ç¬¬åäº”å±Šâ€œè¥¿é—¨å­æ¯â€ä¸­å›½æ™ºèƒ½åˆ¶é€ æŒ‘æˆ˜èµ› ç¦»æ•£è¡Œä¸šè‡ªåŠ¨åŒ–
                </div>
                <div class="award-item">
                    <strong>å† å†›&é˜Ÿé•¿</strong> - æ­¦æ±‰å¸‚2021å¹´"å¾®æ´¾Â·ç§å­æ¯"åˆ›æ–°æ€§è½¯ä»¶ç¼–ç¨‹PKèµ›
                </div>
                <div class="award-item">
                    <strong>å† å†›&é˜Ÿé•¿</strong> - åä¸­ç§‘æŠ€å¤§å­¦ç¬¬åå››å±Šç‘è¨æ¯æ™ºèƒ½è½¦å¤§èµ›
                </div>
                <div class="award-item">
                    <strong>äºŒç­‰å¥–</strong> - å…¨å›½å¤§å­¦ç”Ÿç”µå·¥æ•°å­¦å»ºæ¨¡ã€MathorCupé«˜æ ¡æ•°å­¦å»ºæ¨¡ã€"åä¸­æ¯"å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡
                </div>
            </div>

            <div class="award-category">
                <h3>ğŸ“œ å‘æ˜ä¸“åˆ©</h3>
                <div class="patent-item">
                    <strong>CN202411248996.8</strong> - åŸºäºé¸Ÿç°è§†è§’è¡¨å¾çš„å•ç›®è§†è§‰é‡Œç¨‹è®¡ç³»ç»ŸåŠå…¶æ–¹æ³•ï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
                </div>
                <div class="patent-item">
                    <strong>CN202411981529.6</strong> - åŸºäºé¸Ÿç°è§†è§’è¡¨å¾å’Œå¯å¾®åˆ†åŠ æƒProcrustesæ±‚è§£å™¨çš„å•ç›®è§†è§‰é‡Œç¨‹è®¡æ–¹æ³•å’Œç³»ç»Ÿï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
                </div>
                <div class="patent-item">
                    <strong>CN202110568966.5</strong> - åŸºäºåŒç›®è§†è§‰å’Œæ·±åº¦å­¦ä¹ çš„è½¦è¾†é¿éšœæ–¹æ³•ä¸ç”µå­è®¾å¤‡ï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
                </div>
                <div class="patent-item">
                    <strong>CN202110971772.X</strong> - ä¸€ç§åŸºäºæ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„é«˜é€Ÿè¿åŠ¨è½¦è¾†æ§åˆ¶æ–¹æ³•ï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
                </div>
                <div class="patent-item">
                    <strong>CN202110979800.2</strong> - ä¸€ç§åŒé˜¶æ®µè‡ªåŠ¨é©¾é©¶è½¦è¾†è°ƒå¤´è½¨è¿¹è§„åˆ’æ–¹æ³•ï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
                </div>
            </div>

            <div class="award-category">
                <h3>ğŸŒ± æœ¬ç§‘ç”Ÿåˆ›ä¸šé¡¹ç›®</h3>
                <div class="patent-item">
                    <strong>çœçº§å¤§åˆ›</strong> - "åŸºäºåŒç›®è§†è§‰å’Œæ·±åº¦å­¦ä¹ çš„è½»é‡çº§æ— äººé…é€è½¦"ï¼ˆåˆåˆ›é¡¹ç›®ï¼Œä¼˜ç§€ç»“é¢˜ï¼‰
                </div>
            </div>
        </section>

        <!-- Beyond Research Section -->
        <section id="beyond-research">
            <h2>Beyond Research</h2>
            
            <div class="activity-item">
                <h3>ğŸ« å¹¿è¥¿ç‰æ—-å¤åŸå°å­¦æ”¯æ•™</h3>
                <p>
                    2019 å¹´ç››å¤ï¼Œæˆ‘éšæ”¯æ•™é˜Ÿæ¥åˆ°å¹¿è¥¿ç‰æ—å¤åŸå°å­¦ï¼Œå…¼ä»»è‹±è¯­è€å¸ˆå’Œç­ä¸»ä»»ï¼Œåœ¨æ½®æ¹¿é—·çƒ­ä¸å¿½æ™´å¿½é›¨çš„å¤©æ°”é‡Œï¼Œä¸å‡ åä¸ªæ€§æ ¼è¿¥å¼‚çš„å­©å­ä¸€èµ·åº¦è¿‡äº†éš¾å¿˜çš„ä¸‰å‘¨ã€‚æœ€åˆä»æ…Œä¹±é“ºåºŠã€ç´§å¼ ä¸Šè¯¾ï¼Œåˆ°ç†Ÿæ‚‰æ ¡å›­æ¯ä¸ªè§’è½ã€èƒ½å¤Ÿä»å®¹ç«™ä¸Šè®²å°ï¼Œæˆ‘åœ¨å¤‡è¯¾ã€å¸¦ç­ã€å®¶è®¿å’Œæ— æ•°æ¬¡â€œå´©æºƒ-é‡æ•´â€çš„å¾ªç¯ä¸­ï¼Œæ…¢æ…¢å­¦ä¼šäº†æ‰¿æ‹…ã€è€å¿ƒä¸çœŸæ­£æ„ä¹‰ä¸Šçš„é™ªä¼´ã€‚å­©å­ä»¬ä¸€å¥â€œè€å¸ˆï¼Œä½ è®²å¾—å¥½æœ‰æ„æ€â€ã€å‡ ä»½ç»ˆäºä¸å†æŠ„è¢­çš„ä½œä¸šï¼Œå’Œä»–ä»¬åœ¨æš´é›¨åæ“åœºä¸Šè‚†æ„å¥”è·‘çš„èº«å½±ï¼Œå§‹ç»ˆæ˜¯æˆ‘è·¨è¿‡é«˜çƒ§ã€äº‰æ‰§ä¸è‡ªæˆ‘æ€€ç–‘çš„åŠ›é‡æ¥æºã€‚
                </p>
                <p>
                    å°æ—¶å€™çœ‹è¿‡ä¸€éƒ¨å…³äºæ‹å–çš„çºªå½•ç‰‡ï¼Œä»é‚£æ—¶èµ·ï¼Œæˆ‘ä¾¿ä¸€ç›´æ€€æ£ç€æƒ³è¦è§£å†³æ‹å–çš„é—®é¢˜ã€‚è¿™ä¸ªæ¢¦æƒ³è‡³ä»Šä»é¥è¿œï¼Œä½†åœ¨è¿™æ‰€åè¿œçš„å±±æ‘å°å­¦ï¼Œåœ¨ 2019 å¹´é‚£ä¸ªæ½®æ¹¿çš„å¤å¤©ï¼Œæˆ‘ç¬¬ä¸€æ¬¡çœŸåˆ‡æ„è¯†åˆ°ï¼šæ•™è‚²ä¸å®ˆæŠ¤æ˜¯é¢„é˜²ä¼¤å®³çš„ç¬¬ä¸€é“é˜²çº¿ï¼Œä¹Ÿæ˜¯ä¸ºå­©å­ä»¬åœ¨æ·±å±±ç©¹é¡¶ä¸Šæ‰“å¼€çš„ä¸€æ‰‡å¤©çª—ã€‚ç¦»å¼€å¤åŸå°å­¦åï¼Œæˆ‘ä»å¸¸æƒ³èµ·é‚£äº›åœ¨é»„æ˜ç¯å…‰ä¸‹å†™ä½œä¸šã€åœ¨æœ€åä¸€æ™šçš„æœˆå…‰ä¸‹ä¸æˆ‘é“åˆ«çš„å­©å­ä»¬â€”â€”æ„¿æˆ‘å½“æ—¶ç¬¨æ‹™å´çœŸè¯šçš„åŠªåŠ›ï¼Œèƒ½åœ¨ä»–ä»¬çš„äººç”Ÿé‡Œç•™ä¸‹å“ªæ€•ä¸€ç‚¹ç‚¹æ¸©æš–è€Œé•¿ä¹…çš„å…‰ã€‚
                </p>
            </div>
            
            <div class="activity-item">
                <h3>ğŸ’¼ åˆ›ä¸šç»å†</h3>
                <p>
                    åœ¨æœ¬ç§‘é˜¶æ®µï¼Œæˆ‘ç»„å»ºå¹¶é¢†å¯¼äº†ä¸€æ”¯å…±è®¡11äººçš„åˆ›å§‹å›¢é˜Ÿï¼Œå‘èµ·äº†ä¸€æ¬¾æ™ºèƒ½äººä½“å·¥å­¦æ¤…é¡¹ç›®ï¼Œè‡´åŠ›äºè§£å†³ä¹…åå¸¦æ¥çš„è…°èƒŒå¥åº·é—®é¢˜ã€‚æˆ‘ä»¬ç›®æ ‡æ˜¯é€šè¿‡â€œå¯æ„ŸçŸ¥ã€ä¼šå­¦ä¹ ã€èƒ½ä¸»åŠ¨æ”¯æ’‘â€çš„æ¤…èƒŒç³»ç»Ÿï¼Œæä¾›è¶…è¶Šä¼ ç»Ÿåº§æ¤…çš„ä¸ªæ€§åŒ–æ”¯æ’‘ä½“éªŒã€‚æˆ‘ä¸»å¯¼äº†æ•´ä½“äº§å“è§„åˆ’ä¸æŠ€æœ¯è·¯çº¿è®¾è®¡ï¼Œä»å¸‚åœºè°ƒç ”ã€ç”¨æˆ·éœ€æ±‚åˆ†æï¼Œåˆ°åŠŸèƒ½å®šä¹‰ã€ç³»ç»Ÿæ¶æ„è®¾è®¡ä¸é¡¹ç›®å®æ–½ï¼Œå…¨é¢æ¨åŠ¨äº†æ–¹æ¡ˆçš„è½åœ°å’Œå›¢é˜Ÿçš„ååŒã€‚
                </p>
                <p>
                    æˆ‘ä»¬çš„æ ¸å¿ƒæŠ€æœ¯ç»“åˆäº†å‹åŠ›ä¼ æ„ŸçŸ©é˜µã€èƒŒéƒ¨ä¼ æ„Ÿå™¨ä¸æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿç²¾å‡†è¯†åˆ«ç”¨æˆ·èº«ä»½ã€è‡€éƒ¨ä½ç½®ä¸åå§¿ï¼Œå¹¶æ®æ­¤è‡ªåŠ¨è°ƒæ•´æ”¯æ’‘åŠ›åº¦ã€‚æˆ‘ä»¬çš„æ”¯æ’‘ç³»ç»Ÿé‡‡ç”¨æœºæ¢°æ¨æ†ä¸å¤šåˆ†åŒºæ°”å›Šç»„åˆï¼Œæœºæ¢°éƒ¨åˆ†è´Ÿè´£å¤§èŒƒå›´è°ƒèŠ‚ï¼Œæ°”å›Šåˆ™æä¾›ç²¾ç»†è°ƒæ•´ï¼Œæ”¯æŒå…¨è‡ªåŠ¨ã€åŠè‡ªåŠ¨å’Œæ‰‹åŠ¨è°ƒèŠ‚ã€‚é€šè¿‡Appä¸å®ä½“æŒ‰é”®çš„äº¤äº’ï¼Œç³»ç»Ÿèƒ½ä¸æ–­å­¦ä¹ å¹¶ä¼˜åŒ–ç”¨æˆ·ä¹ æƒ¯ï¼Œå®ç°ä¸ªæ€§åŒ–çš„è¿½èƒŒä¸è…°éª¶éª¨æ”¯æ’‘æ•ˆæœã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘å¸¦é¢†å›¢é˜Ÿè¿ç”¨ç¬¬ä¸€æ€§åŸç†åˆ†æé—®é¢˜ï¼Œå¹¶é€šè¿‡æœ€å°å­ç³»ç»Ÿè¿­ä»£éªŒè¯æ–¹æ¡ˆï¼ŒæˆåŠŸå®Œæˆäº†ä¸‰ä»£åŠŸèƒ½æ ·æœºï¼Œæ¶µç›–äº†ä»ç»“æ„è®¾è®¡ã€æ§åˆ¶ç”µè·¯åˆ°ç®—æ³•éªŒè¯ã€ä¾›åº”é“¾ä¸ä¸“åˆ©å¸ƒå±€çš„å®Œæ•´æµç¨‹ï¼Œå¹¶é¡ºåˆ©è·å¾—äº†å¤©ä½¿è½®èèµ„æœºä¼šã€‚
                </p>
            </div>
        </section>

        <!-- Footer -->
        <footer>
            <p>Â© 2025 é­é›¨é£ (Yufei Wei).</p>
            <p>Last updated: November 2025</p>
        </footer>
    </div>
</body>
</html>
