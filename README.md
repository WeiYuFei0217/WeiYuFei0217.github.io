# é­é›¨é£ (Yufei Wei)

<div align="center">

<img src="profile_photo.jpg" alt="é­é›¨é£" width="250"/>

ğŸ“ **PhD Student @ Zhejiang University**  
ğŸ« **College of Control Science and Engineering**  
ğŸ¤– **3D Vision, World Model & Embodied AI Researcher**

[![Email](https://img.shields.io/badge/Email-wyf00%40zju.edu.cn-blue)](mailto:wyf00@zju.edu.cn)
[![Google Scholar](https://img.shields.io/badge/Google%20Scholar-Profile-green)](https://scholar.google.com/citations?user=68ftKf4AAAAJ)
[![GitHub](https://img.shields.io/badge/GitHub-WeiYuFei0217-black)](https://github.com/WeiYuFei0217)

</div>

---

## ğŸ“– Biography

Hi! I'm Yufei Wei (é­é›¨é£), a Ph.D. student at the College of Control Science and Engineering, Zhejiang University, advised by Prof. Yue Wang and Prof. Rong Xiong. My research centers on 3D visual perception, world models, and embodied AI â€” I'm driven by the vision of enabling robots to genuinely understand and interact with the real world, not just move through it.

My earlier work focused on monocular visual odometry, Bird's-Eye View representations, and visual localization, published in journals and conferences including IEEE RA-L and IROS. These efforts addressed real-world challenges in reducing scale drift and achieving robust localization for mobile robots in complex environments. As a collaborator, I've also contributed to projects spanning learning-based navigation planning, multi-sensor fusion localization, and applications in  agricultural automation and medical robotics. Currently, I'm exploring world model construction and vision-and-language navigation, focusing on cross-modal integration through unified representations, and leveraging multimodal LLMs for scene understanding and navigation tasks.

During my undergraduate years, I led teams to three national championships in autonomous driving and robotics competitions, contributed to the PaddlePaddle open-source community, and developed a lightweight autonomous delivery vehicle. I also served as team leader for an intelligent ergonomic chair startup at Zexiang Li's Entrepreneurship Incubator in Shenzhen and spent time teaching at a primary school in Yulin, Guangxi. During my internship at Shenzhen InnoX Academy, I deployed LiDAR-IMU SLAM and developed obstacle avoidance algorithms for street sweeping vehicles. Whether in research or real-world deployment, I prioritize building systems that actually work over chasing theoretical elegance alone.

I'm always excited to collaborate, discuss ideas, and learn from others. If you're working on visual perception, world models, embodied AI, or anything that brings robots closer to understanding our world, let's chat! ğŸ¤–

---

## ğŸ“° News

- **2025.11** | First-author preprint **Multi-cam Multi-map Visual Inertial Localization** released on arXiv, proposing a multi-camera multi-map visual-inertial localization system with causality-preserving evaluation metrics.

- **2025.09** | First-author preprint **BEV-ODOM2** released on arXiv, exploring PV-BEV fusion and dense flow supervision for BEV-based 3-DoF monocular visual odometry.

- **2025.02** | First-author paper **BEV-DWPVO** accepted by **IEEE RA-L**, achieving best average RTE with superior scale consistency using BEV representation and differentiable weighted Procrustes solver.

- **2024.10** | First-author paper **BEV-ODOM** published at **IROS 2024**, demonstrating that BEV representation alone can achieve low scale drift in monocular visual odometry.

---

## ğŸ“ Education

### Zhejiang University (æµ™æ±Ÿå¤§å­¦)
**Ph.D. Student** | Control Science and Engineering | 2022.09 - 2027.06 (Expected)  
Advisor: Prof. Yue Wang (ç‹è¶Šæ•™æˆ), Prof. Rong Xiong (ç†Šè“‰æ•™æˆ)  
Research Interests: 3D Visual Perception, World Models, Embodied AI

### Huazhong University of Science and Technology (åä¸­ç§‘æŠ€å¤§å­¦)
**B.Eng.** | Electronic Information Engineering | 2018.09 - 2022.06  
School of Electronic Information and Communications

---

## ğŸ“š Publications

> ğŸ“‘ **[My Google Scholar](https://scholar.google.com/citations?user=68ftKf4AAAAJ)**

### First-Author Publications

#### ğŸ“„ BEV-ODOM2: Enhanced BEV-based Monocular Visual Odometry with PV-BEV Fusion and Dense Flow Supervision for Ground Robots

<div align="center">
<img src="paper_images/bev_odom2.jpg" alt="BEV-ODOM2" width="600"/>
</div>

**Authors:** **Yufei Wei**, Wangtao Lu, Sha Lu, Chenxiao Hu, Fuzhang Han, Rong Xiong, Yue Wang  
**Venue:** IEEE Transactions on Intelligent Transportation Systems (T-ITS) - **Under Review**  
**Description:** BEV-ODOM2 introduces dense BEV optical flow supervision constructed directly from pose ground truth and PV-BEV dual-branch fusion to address sparse supervision and information loss without additional annotations. Achieving 40% RTE improvement over existing BEV methods, it delivers state-of-the-art performance across multiple datasets while maintaining superior scale consistency. **(Under Review)**  
[[Paper]](https://arxiv.org/abs/2509.14636)

---

#### ğŸ“„ BEV-DWPVO: BEV-based Differentiable Weighted Procrustes for Low Scale-drift Monocular Visual Odometry on Ground

<div align="center">
<img src="paper_images/bev_dwpvo.jpg" alt="BEV-DWPVO" width="600"/>
</div>

**Authors:** **Yufei Wei**, Sha Lu, Wangtao Lu, Rong Xiong, Yue Wang  
**Venue:** IEEE Robotics and Automation Letters (RA-L), 2025  
**Description:** BEV-DWPVO leverages unified metric-scaled BEV representation and differentiable weighted Procrustes solver to address scale drift in monocular visual odometry. By simplifying pose estimation to 3-DoF and training end-to-end with only pose supervision, it achieves best average RTE of 8.67% and 6.92% on NCLT and Oxford datasets, delivering superior scale consistency without requiring depth estimation or segmentation supervision.  
[[Paper]](https://ieeexplore.ieee.org/abstract/document/10909180) [[Code]](https://github.com/WeiYuFei0217/BEV-DWPVO)

---

#### ğŸ“„ BEV-ODOM: Reducing Scale Drift in Monocular Visual Odometry with BEV Representation

<div align="center">
<img src="paper_images/bev_odom.jpg" alt="BEV-ODOM" width="600"/>
</div>

**Authors:** **Yufei Wei**, Sha Lu, Fuzhang Han, Rong Xiong, Yue Wang  
**Venue:** 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 349-356  
**Description:** BEV-ODOM is the first to demonstrate that BEV representation alone, without segmentation or auxiliary tasks, can achieve low scale drift in monocular visual odometry. Using correlation-based feature extraction and 3-DoF pose regression with only pose supervision, it reveals that scale consistency originates from BEV's inherent metric-scaled grid structure rather than side-tasks, providing a simpler and more interpretable approach.  
[[Paper]](https://ieeexplore.ieee.org/abstract/document/10801996)

---

#### ğŸ“„ Multi-cam Multi-map Visual Inertial Localization: System, Validation and Dataset

<div align="center">
<img src="paper_images/multi_cam.jpg" alt="Multi-cam Multi-map VIL" width="600"/>
</div>

**Authors:** **Yufei Wei**, Fuzhang Han, Yanmei Jiao, Zhuqing Zhang, Yiyuan Pan, Wenjun Huang, Li Tang, Huan Yin, Xiaqing Ding, Rong Xiong, Yue Wang  
**Venue:** IEEE Transactions on Field Robotics (T-FR) - **Under Review**  
**Description:** Robot control requires causal pose estimates without retroactive corrections. This work proposes a multi-camera filter-based VILO system enabling operation across multiple isolated maps without overlap requirements. Deterministic initialization and IMU-aided 2-point minimal solvers maintain robustness under 80%+ outlier rates. To validate the system under long-term appearance changes, we collect a 9-month, 55km campus dataset and propose causality-preserving evaluation metrics aligned with control requirements.  
[[Paper]](https://arxiv.org/abs/2412.04287) [[Code]](https://github.com/WeiYuFei0217/BEV-DWPVO)

---

### Co-authored Publications

#### ğŸ“„ Learning to Tune a Mobile Robot Planner: Formulation, Architecture, and Sim-to-Real

<div align="center">
<img src="paper_images/LWT_JFR2026.jpg" alt="Learning to Tune" width="600"/>
</div>

**Authors:** Wangtao Lu, Wei Zhang, **Yufei Wei**, Rong Xiong, Chaoqun Wang, Yue Wang  
**Venue:** Journal of Field Robotics - **Under Review**  
**Description:** This work resolves the architectural bottleneck of learning-based planner tuning by proposing a multi-rate hierarchical framework that decouples navigation into coordinated low-frequency tuning (1Hz), mid-frequency planning (10Hz), and high-frequency control (50Hz) loops. The Cyclic Co-Training strategy enables stable learning of coupled policies, while the Terrain-Adaptive Controller achieves robust zero-shot sim-to-real transfer by maintaining consistent tracking performance across diverse terrains, outperforming state-of-the-art auto-tuning and end-to-end methods. **(Under Review)**

---

#### ğŸ“„ Human-Guided Robotic-Assistance Handheld Continuum Medical Robot System

<div align="center">
<img src="paper_images/WF_IROS2025.jpg" alt="HRHC" width="600"/>
</div>

**Authors:** Fei Wang, Changhao Luo, Zexi Zhao, Pingyu Xiang, **Yufei Wei**, Ke Qiu, Yufei Wei, Yue Wang, Rong Xiong and Haojian Lu  
**Venue:** 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) - Accepted  
**Description:** HRHC presents a human-guided handheld continuum medical robot system that bridges the gap between manual instruments and fully robotic solutions for minimally invasive surgery. Featuring modular design with integrated binocular vision for real-time depth perception, IMU-driven intuitive control, and millimeter-level dexterity in confined spaces, it extends surgeon capabilities while maintaining portability and natural operation. My contribution: sensor system architecture design and software deployment for the multi-modal perception platform.

---

#### ğŸ“„ SeqBEV: Bayesian Sequential Localization Using BEV LiDAR Map

<div align="center">
<img src="paper_images/LS_ACAR2025.jpg" alt="SeqBEV" width="600"/>
</div>

**Authors:** Sha Lu, **Yufei Wei**, Rong Xiong, Yue Wang  
**Venue:** 2025 IEEE International Conference on Real-time Computing and Robotics (RCAR), pp. 407-412  
**Description:** SeqBEV proposes a robust sequential LiDAR localization framework using compact BEV map representation and Bayesian filtering for challenging sparse map scenarios (50m node spacing). By employing NCC-based template matching for uncertainty-aware pose distribution estimation and recursive Bayesian fusion for temporal consistency, it achieves 23.33% improvement in Recall@1 and reduces localization errors to 0.62m (50th percentile) on the NCLT dataset, demonstrating superior robustness compared to single-shot and descriptor-based methods in feature-insufficient environments.  
[[Paper]](https://ieeexplore.ieee.org/abstract/document/11139435)

---

#### ğŸ“„ Reinforcement Learning for Adaptive Planner Parameter Tuning: A Perspective on Hierarchical Architecture

<div align="center">
<img src="paper_images/LWT_ICRA2025.jpg" alt="RL Planner Tuning" width="600"/>
</div>

**Authors:** Wangtao Lu, **Yufei Wei**, Jiadong Xu, Wenhao Jia, Liang Li, Rong Xiong, Yue Wang  
**Venue:** 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 3883-3889  
**Description:** This work proposes a hierarchical architecture integrating low-frequency parameter tuning (1Hz), mid-frequency planning (10Hz), and high-frequency control (50Hz) for autonomous navigation. An RL-based feedback controller is introduced to minimize tracking errors that cannot be resolved through parameter tuning alone. Through alternating training between the parameter tuning network and controller, the method achieves 98% success rate and 10.2s completion time on BARN Challenge, winning first place. Real-world experiments demonstrate 100% success rate with superior tracking accuracy.  
[[Paper]](https://ieeexplore.ieee.org/abstract/document/11128541)

---

#### ğŸ“„ Towards Agricultural Vehicle Autonomy: Adaptive PID Control for Precise Path Tracking with Visual-Inertial Localization

<div align="center">
<img src="paper_images/ZDK_CCC2025.jpg" alt="Agricultural Vehicle" width="600"/>
</div>

**Authors:** Dongkun Zhang, **Yufei Wei**, Xujiong Feng, Jinpeng Gan, Zhi Huang, Huanyu Jiang, Zidong Yang, Daxu Zhao, Rong Xiong, Yue Wang  
**Venue:** 2025 44th Chinese Control Conference (CCC), pp. 4630-4636  
**Description:** This work presents a cost-effective autonomous agricultural vehicle system using visual-inertial localization to replace expensive RTK-GPS. The proposed modular design integrates a quad-camera setup with IMU for robust pose estimation and employs an adaptive PID controller based on FOPTD model for precise path tracking. Field experiments demonstrate errors below 4.38cm and over 90% trajectory points within 7.5cm threshold using vision-only localization, validating its feasibility for practical agricultural automation.  
[[Paper]](https://ieeexplore.ieee.org/abstract/document/11178414)

---

#### ğŸ“„ Demonstration Data-Driven Parameter Adjustment for Trajectory Planning in Highly Constrained Environments

<div align="center">
<img src="paper_images/LWT_RAL2025.jpg" alt="Demonstration Data-Driven" width="600"/>
</div>

**Authors:** Wangtao Lu, Lin Chen, Yunkai Wang, **Yufei Wei**, Zifei Wu, Rong Xiong, Yue Wang  
**Venue:** IEEE Robotics and Automation Letters (RA-L), 2024  
**Description:** This work introduces a demonstration data-driven RL framework using CGAN discriminator to bridge trajectory-space demonstrations and parameter-space learning. Combined with asynchronous controller architecture, the method achieves 5Ã— faster convergence than pure RL (0.6M vs 3M steps) and secures first place in BARN Challenge, with strong sim-to-real transfer capability validated in real-world experiments.  
[[Paper]](https://ieeexplore.ieee.org/abstract/document/10749994)

---

#### ğŸ“„ VIVO: A Visual-Inertial-Velocity Odometry with Online Calibration in Challenging Condition

<div align="center">
<img src="paper_images/HFZ_IROS2024.jpg" alt="VIVO" width="600"/>
</div>

**Authors:** Fuzhang Han, Shenhan Jia, Jiyu Yu, **Yufei Wei**, Wenjun Huang, Yue Wang, Rong Xiong  
**Venue:** 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 8571-8578  
**Description:** VIVO proposes a tightly coupled visual-inertial-velocity odometry with online extrinsic calibration, directly fusing high-frequency velocity measurements into MSCKF-based VIO. Validated on both wheeled robots and high-speed quadrupeds (up to 3 m/s), it achieves superior robustness in challenging conditions with lower ATE than OpenVINS/MSF and 2Ã— efficiency over ORB-SLAM3.  
[[Paper]](https://ieeexplore.ieee.org/abstract/document/10801486)

---

#### ğŸ“„ OTVIC: A Dataset with Online Transmission for Vehicle-to-Infrastructure Cooperative 3D Object Detection

<div align="center">
<img src="paper_images/ZH_IROS2024.jpg" alt="OTVIC" width="600"/>
</div>

**Authors:** He Zhu, Yunkai Wang, Quyu Kong, **Yufei Wei**, Xunlong Xia, Bing Deng, Rong Xiong, Yue Wang  
**Venue:** 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 10732-10739  
**Description:** OTVIC presents the first real-world V2I cooperative perception dataset with online transmission, capturing dynamic delays, high-speed scenarios (70-110 km/h), and bandwidth constraints in highway environments. The proposed LfFormer framework achieves robust late fusion by encoding infrastructure perception results as transformer queries, delivering 2.3% mAP improvement while maintaining low communication bandwidth and strong robustness to delays and packet loss.  
[[Paper]](https://ieeexplore.ieee.org/abstract/document/10802656)

---

#### ğŸ“„ Adapting for Calibration Disturbances: A Neural Uncalibrated Visual Servoing Policy

<div align="center">
<img src="paper_images/YHX_ICRA2024.jpg" alt="NUVS" width="600"/>
</div>

**Authors:** Hongxiang Yu, Anzhe Chen, Kechun Xu, Dashun Guo, **Yufei Wei**, Zhongxiang Zhou, Xuebo Zhang, Yue Wang, Rong Xiong  
**Venue:** 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 17417-17423  
**Description:** NUVS addresses the labor-intensive calibration challenge in industrial visual servoing by proposing a neural uncalibrated policy that adapts to camera calibration disturbances. By estimating calibration embeddings from past observations to modulate the neural controller and leveraging PBVS supervision in simulation, it achieves both the disturbance adaptation of classical methods and the large convergence basin of learning-based approaches, outperforming IBUVS under calibration disturbances with large initial pose offsets.  
[[Paper]](https://ieeexplore.ieee.org/abstract/document/10610364)

---

## ğŸ”¬ Research Projects

### ğŸš€ ZJU & xx Research Institute - Legged xx Exploration Robot Project
Designed and led the development of a full-stack perception and navigation system for autonomous exploration in complex terrain. Built an integrated multi-sensor stack (LiDAR, stereo cameras, IMU) with a robust calibration pipeline, and implemented laser-inertial / visual-inertial localization together with deep learningâ€“based stereo matching for dense 3D reconstruction. On top of this stack, developed a lightweight terrain analysis and path-planning framework tailored to resource-constrained platforms, operating directly on point clouds to extract ground planes, characterize terrain geometry (concavities/convexities), identify traversable regions, and generate optimized paths via graph-based planning. Validated the system in 100+ automated test scenarios, demonstrating reliable navigation performance in cluttered, highly irregular environments.

*\* xx: redacted for confidentiality*

---

### ğŸŒ¾ Provincial Major R&D Program - Agricultural Automation System
Designed and led the development of a vision-only navigation system for agricultural machinery, covering the complete sensing architecture, perception stack, and visualâ€“inertial odometry framework. Built a four-camera and IMU perception module with a dedicated vibration-attenuating structure and implemented a multi-view visualâ€“inertial fusion system optimized for low-texture, high-vibration agricultural environments. Integrated the system with an error-compensation tracking controller and demonstrated centimeter-level trajectory accuracy in rice-transplanter farmland trials when compared against RTK-INS ground truth.

---

### ğŸ¦¾ ZJU & ZJH (Zhejiang Humanoid Robot Innovation Center) â€“ Wheeled Humanoid Perception & Navigation System
Developed the perception and navigation subsystem for a wheeled humanoid robot platform. Built a vision-centric BEV representation from multi-camera inputs to generate dense costmaps and 3-DoF pose estimates. Integrated the perception layer with A* global planning and TEB local optimization, enabling reliable, obstacle-aware visual navigation for the humanoid robot's wheeled base.

---

### ğŸš™ ZJU & Alibaba - Autonomous Driving Data Collection Platform
Designed the perception hardware architecture for the autonomous driving data collection vehicle, including structural design and vehicle-level integration of the quad-camera, LiDAR, IMU, GPS/RTK, and V2X sensing modules. Implemented the multi-sensor calibration workflow and deployed the multi-view visualâ€“IMUâ€“GPS odometry algorithms on the vehicle platform to ensure accurate and reliable motion estimation. Conducted data consistency checks and ground-truth alignment to support high-quality multi-modal dataset generation.

---

### ğŸ• ZJU & SDU - Quadruped Robot Swarm Project
Deployed stereo-visionâ€“based 3D reconstruction on quadruped platforms and built a BEV-centric spatial representation for robust perception in multi-robot settings. Implemented BEV odometry to generate reliable pose estimates for downstream planning modules, enabling coordinated motion within the robot swarm.

---

## ğŸ’¼ Industrial Experience

### ğŸ¢ Shenzhen InnoX Academy - Autonomous Driving Center (2022.2 - 2022.9)
Built a semantic line segment mapping system for autonomous street sweeping vehicles in urban environments. The system uses deep semantic segmentation to extract static linear landmarks like lamp posts and road markings. By representing these landmarks as compact line segments rather than dense point clouds, it filters out dynamic traffic participants, handles varying weather and lighting conditions, and enables real-time LiDAR-IMU SLAM on resource-constrained onboard computers.

---

## ğŸ† ç«èµ›è·å¥–ã€ä¸“åˆ©ä¸é¡¹ç›®

### ğŸ† ç«èµ›è·å¥–

- **å…¨å›½å† å†›&é˜Ÿé•¿** - 2021å¹´å…¨å›½å¤§å­¦ç”Ÿå·¥ç¨‹å®è·µä¸åˆ›æ–°èƒ½åŠ›å¤§èµ› æ™ºèƒ½ç½‘è”æ±½è½¦è®¾è®¡èµ›é“
- **å…¨å›½å† å†›&é˜Ÿé•¿** - ç¬¬åäº”å±Šå…¨å›½å¤§å­¦ç”Ÿæ™ºèƒ½æ±½è½¦ç«èµ› ç™¾åº¦æ™ºèƒ½é©¾é©¶ç»„
- **å…¨å›½å† å†›&é˜Ÿé•¿** - ç¬¬ä¸‰å±Šä¸­å›½é«˜æ ¡æ™ºèƒ½æœºå™¨äººåˆ›æ„å¤§èµ› ROSæ— äººæœºç»„
- **å…¨å›½ä¸€ç­‰å¥–&é˜Ÿé•¿** - ç¬¬äºŒåä¸‰å±Šä¸­å›½æœºå™¨äººåŠäººå·¥æ™ºèƒ½å¤§èµ› æ— äººé©¾é©¶æŒ‘æˆ˜èµ› & æ·±åº¦å­¦ä¹ æ™ºèƒ½è½¦ï¼ˆåŒèµ›é¡¹ï¼‰
- **çœèµ›ä¸€ç­‰å¥–&é˜Ÿé•¿** - 2021"è¥¿é—¨å­æ¯"ä¸­å›½æ™ºèƒ½åˆ¶é€ æŒ‘æˆ˜èµ› ç¦»æ•£è¡Œä¸šè‡ªåŠ¨åŒ–
- **å† å†›&é˜Ÿé•¿** - 2021"å¾®æ´¾Â·ç§å­æ¯"åˆ›æ–°æ€§è½¯ä»¶ç¼–ç¨‹PKèµ›
- **å† å†›&é˜Ÿé•¿** - åä¸­ç§‘æŠ€å¤§å­¦ç¬¬åå››å±Šç‘è¨æ¯æ™ºèƒ½è½¦å¤§èµ›
- **äºŒç­‰å¥–** - å…¨å›½å¤§å­¦ç”Ÿç”µå·¥æ•°å­¦å»ºæ¨¡ã€MathorCupé«˜æ ¡æ•°å­¦å»ºæ¨¡ã€"åä¸­æ¯"å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡

### ğŸ“œ å‘æ˜ä¸“åˆ©

- **CN202411248996.8** - åŸºäºé¸Ÿç°è§†è§’è¡¨å¾çš„å•ç›®è§†è§‰é‡Œç¨‹è®¡ç³»ç»ŸåŠå…¶æ–¹æ³•ï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
- **CN202411981529.6** - åŸºäºé¸Ÿç°è§†è§’è¡¨å¾å’Œå¯å¾®åˆ†åŠ æƒProcrustesæ±‚è§£å™¨çš„å•ç›®è§†è§‰é‡Œç¨‹è®¡æ–¹æ³•å’Œç³»ç»Ÿï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
- **CN202110568966.5** - åŸºäºåŒç›®è§†è§‰å’Œæ·±åº¦å­¦ä¹ çš„è½¦è¾†é¿éšœæ–¹æ³•ä¸ç”µå­è®¾å¤‡ï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
- **CN202110971772.X** - ä¸€ç§åŸºäºæ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„é«˜é€Ÿè¿åŠ¨è½¦è¾†æ§åˆ¶æ–¹æ³•ï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰
- **CN202110979800.2** - ä¸€ç§åŒé˜¶æ®µè‡ªåŠ¨é©¾é©¶è½¦è¾†è°ƒå¤´è½¨è¿¹è§„åˆ’æ–¹æ³•ï¼ˆç¬¬ä¸€å­¦ç”Ÿå‘æ˜äººï¼‰

### ğŸŒ± æœ¬ç§‘ç”Ÿåˆ›ä¸šé¡¹ç›®

- **çœçº§å¤§åˆ›** - "åŸºäºåŒç›®è§†è§‰å’Œæ·±åº¦å­¦ä¹ çš„è½»é‡çº§æ— äººé…é€è½¦"ï¼ˆåˆåˆ›é¡¹ç›®ï¼Œä¼˜ç§€ç»“é¢˜ï¼‰

---

## ğŸŒŸ Beyond Research

### ğŸ« å¹¿è¥¿ç‰æ—-å¤åŸå°å­¦æ”¯æ•™

2019 å¹´ç››å¤ï¼Œæˆ‘éšæ”¯æ•™é˜Ÿæ¥åˆ°å¹¿è¥¿ç‰æ—å¤åŸå°å­¦ï¼Œå…¼ä»»è‹±è¯­è€å¸ˆå’Œç­ä¸»ä»»ï¼Œåœ¨æ½®æ¹¿é—·çƒ­ä¸å¿½æ™´å¿½é›¨çš„å¤©æ°”é‡Œï¼Œä¸å‡ åä¸ªæ€§æ ¼è¿¥å¼‚çš„å­©å­ä¸€èµ·åº¦è¿‡äº†éš¾å¿˜çš„ä¸‰å‘¨ã€‚æœ€åˆä»æ…Œä¹±é“ºåºŠã€ç´§å¼ ä¸Šè¯¾ï¼Œåˆ°ç†Ÿæ‚‰æ ¡å›­æ¯ä¸ªè§’è½ã€èƒ½å¤Ÿä»å®¹ç«™ä¸Šè®²å°ï¼Œæˆ‘åœ¨å¤‡è¯¾ã€å¸¦ç­ã€å®¶è®¿å’Œæ— æ•°æ¬¡"å´©æºƒ-é‡æ•´"çš„å¾ªç¯ä¸­ï¼Œæ…¢æ…¢å­¦ä¼šäº†æ‰¿æ‹…ã€è€å¿ƒä¸çœŸæ­£æ„ä¹‰ä¸Šçš„é™ªä¼´ã€‚å­©å­ä»¬ä¸€å¥"è€å¸ˆï¼Œä½ è®²å¾—å¥½æœ‰æ„æ€"ã€å‡ ä»½ç»ˆäºä¸å†æŠ„è¢­çš„ä½œä¸š,å’Œä»–ä»¬åœ¨æš´é›¨åæ“åœºä¸Šè‚†æ„å¥”è·‘çš„èº«å½±ï¼Œå§‹ç»ˆæ˜¯æˆ‘è·¨è¿‡é«˜çƒ§ã€äº‰æ‰§ä¸è‡ªæˆ‘æ€€ç–‘çš„åŠ›é‡æ¥æºã€‚

å°æ—¶å€™çœ‹è¿‡ä¸€éƒ¨å…³äºæ‹å–çš„çºªå½•ç‰‡ï¼Œä»é‚£æ—¶èµ·ï¼Œæˆ‘ä¾¿ä¸€ç›´æ€€æ£ç€æƒ³è¦è§£å†³æ‹å–çš„é—®é¢˜ã€‚è¿™ä¸ªæ¢¦æƒ³è‡³ä»Šä»é¥è¿œï¼Œä½†åœ¨è¿™æ‰€åè¿œçš„å±±æ‘å°å­¦ï¼Œåœ¨ 2019 å¹´é‚£ä¸ªæ½®æ¹¿çš„å¤å¤©ï¼Œæˆ‘ç¬¬ä¸€æ¬¡çœŸåˆ‡æ„è¯†åˆ°ï¼šæ•™è‚²ä¸å®ˆæŠ¤æ˜¯é¢„é˜²ä¼¤å®³çš„ç¬¬ä¸€é“é˜²çº¿ï¼Œä¹Ÿæ˜¯ä¸ºå­©å­ä»¬åœ¨æ·±å±±ç©¹é¡¶ä¸Šæ‰“å¼€çš„ä¸€æ‰‡å¤©çª—ã€‚ç¦»å¼€å¤åŸå°å­¦åï¼Œæˆ‘ä»å¸¸æƒ³èµ·é‚£äº›åœ¨é»„æ˜ç¯å…‰ä¸‹å†™ä½œä¸šã€åœ¨æœ€åä¸€æ™šçš„æœˆå…‰ä¸‹ä¸æˆ‘é“åˆ«çš„å­©å­ä»¬â€”â€”æ„¿æˆ‘å½“æ—¶ç¬¨æ‹™å´çœŸè¯šçš„åŠªåŠ›ï¼Œèƒ½åœ¨ä»–ä»¬çš„äººç”Ÿé‡Œç•™ä¸‹å“ªæ€•ä¸€ç‚¹ç‚¹æ¸©æš–è€Œé•¿ä¹…çš„å…‰ã€‚

---

### ğŸ’¼ åˆ›ä¸šç»å†

åœ¨æœ¬ç§‘é˜¶æ®µï¼Œæˆ‘ç»„å»ºå¹¶é¢†å¯¼äº†ä¸€æ”¯å…±è®¡11äººçš„åˆ›å§‹å›¢é˜Ÿï¼Œå‘èµ·äº†ä¸€æ¬¾æ™ºèƒ½äººä½“å·¥å­¦æ¤…é¡¹ç›®ï¼Œè‡´åŠ›äºè§£å†³ä¹…åå¸¦æ¥çš„è…°èƒŒå¥åº·é—®é¢˜ã€‚æˆ‘ä»¬ç›®æ ‡æ˜¯é€šè¿‡"å¯æ„ŸçŸ¥ã€ä¼šå­¦ä¹ ã€èƒ½ä¸»åŠ¨æ”¯æ’‘"çš„æ¤…èƒŒç³»ç»Ÿï¼Œæä¾›è¶…è¶Šä¼ ç»Ÿåº§æ¤…çš„ä¸ªæ€§åŒ–æ”¯æ’‘ä½“éªŒã€‚æˆ‘ä¸»å¯¼äº†æ•´ä½“äº§å“è§„åˆ’ä¸æŠ€æœ¯è·¯çº¿è®¾è®¡ï¼Œä»å¸‚åœºè°ƒç ”ã€ç”¨æˆ·éœ€æ±‚åˆ†æï¼Œåˆ°åŠŸèƒ½å®šä¹‰ã€ç³»ç»Ÿæ¶æ„è®¾è®¡ä¸é¡¹ç›®å®æ–½ï¼Œå…¨é¢æ¨åŠ¨äº†æ–¹æ¡ˆçš„è½åœ°å’Œå›¢é˜Ÿçš„ååŒã€‚

æˆ‘ä»¬çš„æ ¸å¿ƒæŠ€æœ¯ç»“åˆäº†å‹åŠ›ä¼ æ„ŸçŸ©é˜µã€èƒŒéƒ¨ä¼ æ„Ÿå™¨ä¸æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿç²¾å‡†è¯†åˆ«ç”¨æˆ·èº«ä»½ã€è‡€éƒ¨ä½ç½®ä¸åå§¿ï¼Œå¹¶æ®æ­¤è‡ªåŠ¨è°ƒæ•´æ”¯æ’‘åŠ›åº¦ã€‚æˆ‘ä»¬çš„æ”¯æ’‘ç³»ç»Ÿé‡‡ç”¨æœºæ¢°æ¨æ†ä¸å¤šåˆ†åŒºæ°”å›Šç»„åˆï¼Œæœºæ¢°éƒ¨åˆ†è´Ÿè´£å¤§èŒƒå›´è°ƒèŠ‚ï¼Œæ°”å›Šåˆ™æä¾›ç²¾ç»†è°ƒæ•´ï¼Œæ”¯æŒå…¨è‡ªåŠ¨ã€åŠè‡ªåŠ¨å’Œæ‰‹åŠ¨è°ƒèŠ‚ã€‚é€šè¿‡Appä¸å®ä½“æŒ‰é”®çš„äº¤äº’ï¼Œç³»ç»Ÿèƒ½ä¸æ–­å­¦ä¹ å¹¶ä¼˜åŒ–ç”¨æˆ·ä¹ æƒ¯ï¼Œå®ç°ä¸ªæ€§åŒ–çš„è¿½èƒŒä¸è…°éª¶éª¨æ”¯æ’‘æ•ˆæœã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘å¸¦é¢†å›¢é˜Ÿè¿ç”¨ç¬¬ä¸€æ€§åŸç†åˆ†æé—®é¢˜ï¼Œå¹¶é€šè¿‡æœ€å°å­ç³»ç»Ÿè¿­ä»£éªŒè¯æ–¹æ¡ˆï¼ŒæˆåŠŸå®Œæˆäº†ä¸‰ä»£åŠŸèƒ½æ ·æœºï¼Œæ¶µç›–äº†ä»ç»“æ„è®¾è®¡ã€æ§åˆ¶ç”µè·¯åˆ°ç®—æ³•éªŒè¯ã€ä¾›åº”é“¾ä¸ä¸“åˆ©å¸ƒå±€çš„å®Œæ•´æµç¨‹ï¼Œå¹¶é¡ºåˆ©è·å¾—äº†å¤©ä½¿è½®èèµ„æœºä¼šã€‚

---

<div align="center">

**Â© 2025 é­é›¨é£ (Yufei Wei)**  
*Last updated: November 2025*

</div>
